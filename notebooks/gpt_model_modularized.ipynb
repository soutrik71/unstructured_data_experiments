{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "IPython.notebook.set_autosave_interval(300000)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 300 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 300\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/batch/tasks/shared/LS_root/mounts/clusters/copilot-model-run/code/Users/Soutrik.Chowdhury/unstructured_data_experiments\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\n",
    "    \"/mnt/batch/tasks/shared/LS_root/mounts/clusters/copilot-model-run/code/Users/Soutrik.Chowdhury/unstructured_data_experiments\"\n",
    ")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2image import convert_from_path\n",
    "import os\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import base64\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "from langchain.schema import Document\n",
    "import re\n",
    "import shutil\n",
    "from joblib import Parallel, delayed\n",
    "from functools import partial\n",
    "from requests.exceptions import RequestException\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(find_dotenv(\"dev.env\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64a128e1a82f40888c77f200c6e5e661\n",
      "https://brewdatgbgaighqtechopenai01ncud.openai.azure.com/\n",
      "2024-02-15-preview\n"
     ]
    }
   ],
   "source": [
    "print(os.getenv(\"AZURE_OPENAI_API_KEY\"))\n",
    "print(os.getenv(\"AZURE_OPENAI_ENDPOINT\"))\n",
    "print(os.getenv(\"OPENAI_API_VERSION\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pdf_to_images(pdf_path, dest_folder, image_format=\"JPEG\"):\n",
    "    \"\"\"\n",
    "    Converts each page of a PDF into images and saves them in a directory.\n",
    "\n",
    "    Args:\n",
    "    - pdf_path (str): Path to the PDF file.\n",
    "    - dest_folder (str): Destination folder to save the images.\n",
    "    - image_format (str): Format to save the images (default is \"JPEG\").\n",
    "\n",
    "    Returns:\n",
    "    - list: List of image file paths saved.\n",
    "    \"\"\"\n",
    "\n",
    "    pdf_name = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "    output_dir = os.path.join(dest_folder, f\"{pdf_name}_images\")\n",
    "    if os.path.exists(output_dir):\n",
    "        # remove the existing directory\n",
    "        shutil.rmtree(output_dir)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    images = convert_from_path(pdf_path)\n",
    "    saved_image_paths = []\n",
    "\n",
    "    for i, img in enumerate(images):\n",
    "        image_path = os.path.join(output_dir, f\"page{i}.{image_format.lower()}\")\n",
    "        img.save(image_path, image_format)\n",
    "        saved_image_paths.append(image_path)\n",
    "\n",
    "    return saved_image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(image_path):\n",
    "    \"\"\"\n",
    "    Encodes an image to base64.\n",
    "\n",
    "    Args:\n",
    "    - image_path (str): Path to the image file.\n",
    "\n",
    "    Returns:\n",
    "    - str: Base64 encoded string of the image.\n",
    "    \"\"\"\n",
    "    with open(image_path, \"rb\") as img_file:\n",
    "        encoded_image = base64.b64encode(img_file.read()).decode(\"utf-8\")\n",
    "    return encoded_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpt_model_infra(api_key, endpoint, model_name, api_version):\n",
    "    \"\"\"\n",
    "    Sets up headers and endpoint for GPT model API.\n",
    "\n",
    "    Args:\n",
    "    - api_key (str): API key for authentication.\n",
    "    - endpoint (str): Base URL of the API endpoint.\n",
    "    - model_name (str): Name of the GPT model (default is \"gpt-4o\").\n",
    "    - api_version (str): API version (default is \"2024-02-15-preview\").\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Headers and GPT model endpoint.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"api-key\": api_key,\n",
    "    }\n",
    "\n",
    "    gpt_endpoint = f\"{endpoint}openai/deployments/{model_name}/chat/completions?api-version={api_version}\"\n",
    "    # print(gpt_endpoint)\n",
    "    return headers, gpt_endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpt_model_payload(encoded_image, temperature=0.0, top_p=1e-5, seed=1234):\n",
    "    \"\"\"\n",
    "    Constructs the payload for the GPT model API request.\n",
    "\n",
    "    Args:\n",
    "    - encoded_image (str): Base64 encoded image string.\n",
    "    - temperature (float): Sampling temperature (default is 0.0).\n",
    "    - top_p (float): Cumulative probability for nucleus sampling (default is 1e-5).\n",
    "    - seed (int): Seed for reproducibility (default is 1234).\n",
    "\n",
    "    Returns:\n",
    "    - dict: Payload for the GPT model API request.\n",
    "    \"\"\"\n",
    "    # this can be argument to the function\n",
    "    system_message = (\n",
    "        \"You are an AI assistant specializing in extracting information from images containing three main components: \"\n",
    "        \"text, tables, and figures. Your tasks are as follows:\\n\"\n",
    "        \"* Extract the text exactly as it appears in the image.\\n\"\n",
    "        \"* Extract data from tables while preserving the table structure, providing the data in a structured 2D format, \"\n",
    "        \"and maintaining hierarchical columns if present.\\n\"\n",
    "        \"* Extract and describe the information from figures in plain text format, making it understandable and reproducible \"\n",
    "        \"from the extracted data.\\n\\n\"\n",
    "        \"The final output should be in plain text format, containing only the extracted information.\"\n",
    "    )\n",
    "\n",
    "    user_message = {\n",
    "        \"type\": \"image_url\",\n",
    "        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{encoded_image}\"},\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": [user_message]},\n",
    "        ],\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": top_p,\n",
    "        \"seed\": seed,\n",
    "    }\n",
    "\n",
    "    return payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_gpt_model_response(headers, endpoint, payload):\n",
    "    \"\"\"\n",
    "    Sends a request to the GPT model API and retrieves the response.\n",
    "\n",
    "    Args:\n",
    "    - headers (dict): Headers for the API request.\n",
    "    - endpoint (str): Full URL of the GPT model endpoint.\n",
    "    - payload (dict): Payload for the API request.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Response from the GPT model API.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.post(endpoint, headers=headers, json=payload)\n",
    "        response.raise_for_status()\n",
    "    except Exception as e:\n",
    "        raise SystemExit(f\"Failed to make the request. Error: {e}\")\n",
    "\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_image(\n",
    "    image_path, api_key, endpoint, model_name, api_version, max_retries=3, retry_wait=2\n",
    "):\n",
    "    \"\"\"\n",
    "    Extracts text and information from an image using GPT model with manual retry logic.\n",
    "\n",
    "    Args:\n",
    "    - image_path (str): Path to the image file.\n",
    "    - api_key (str): API key for authentication.\n",
    "    - endpoint (str): Base URL of the API endpoint.\n",
    "    - model_name (str): Name of the GPT model.\n",
    "    - api_version (str): API version.\n",
    "    - max_retries (int): Maximum number of retries (default is 3).\n",
    "    - retry_wait (int): Wait time between retries in seconds (default is 2).\n",
    "\n",
    "    Returns:\n",
    "    - dict: GPT model's response containing extracted information.\n",
    "    \"\"\"\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            encoded_image = encode_image(image_path)\n",
    "            headers, gpt_endpoint = get_gpt_model_infra(\n",
    "                api_key, endpoint, model_name, api_version\n",
    "            )\n",
    "            payload = get_gpt_model_payload(encoded_image)\n",
    "            response = invoke_gpt_model_response(headers, gpt_endpoint, payload)\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {retries + 1} failed with error: {e}\")\n",
    "            retries += 1\n",
    "            if retries < max_retries:\n",
    "                time.sleep(retry_wait)\n",
    "            else:\n",
    "                # If the retry attempts fail, return custom fallback text\n",
    "                return {\n",
    "                    \"choices\": [\n",
    "                        {\n",
    "                            \"message\": {\n",
    "                                \"content\": f\"Failed to extract information from {image_path} after {max_retries} attempts.\"\n",
    "                            }\n",
    "                        }\n",
    "                    ],\n",
    "                    \"usage\": {\"completion_tokens\": 0},\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lc_document_from_response(response, image_path):\n",
    "    \"\"\"\n",
    "    Creates a LangChain Document object from the GPT model's response.\n",
    "\n",
    "    Args:\n",
    "    - response (dict): Response from the GPT model.\n",
    "    - image_path (str): Path to the image file.\n",
    "\n",
    "    Returns:\n",
    "    - Document: LangChain Document object containing extracted content and metadata.\n",
    "    \"\"\"\n",
    "    pg_content = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    token_size = response[\"usage\"][\"completion_tokens\"]\n",
    "    curr_time = datetime.now()\n",
    "    pdf_name = re.findall(r\"(.*)_images\\b\", image_path.split(\"/\")[-2])[0]\n",
    "    page_num = re.findall(r\"page(\\d+).jpeg\", image_path.split(\"/\")[-1])[0]\n",
    "\n",
    "    # langchain document object\n",
    "\n",
    "    doc = Document(\n",
    "        page_content=pg_content,\n",
    "        metadata={\n",
    "            \"source\": pdf_name,\n",
    "            \"page_number\": int(page_num),\n",
    "            \"token_size\": token_size,\n",
    "            \"timestamp\": str(curr_time),\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_image_extraction_pipeline(\n",
    "    image_path, api_key, endpoint, model_name, api_version\n",
    "):\n",
    "    \"\"\"\n",
    "    Orchestrates the entire image extraction pipeline from an image to a LangChain Document.\n",
    "\n",
    "    Args:\n",
    "    - image_path (str): Path to the image file.\n",
    "    - api_key (str): API key for authentication.\n",
    "    - endpoint (str): Base URL of the API endpoint.\n",
    "\n",
    "    Returns:\n",
    "    - Document: LangChain Document object containing extracted content and metadata.\n",
    "    \"\"\"\n",
    "    print(f\"Extracting information from {image_path}...\")\n",
    "    response = extract_text_from_image(\n",
    "        image_path, api_key, endpoint, model_name, api_version\n",
    "    )\n",
    "    doc = lc_document_from_response(response, image_path)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images saved to: ['image_extracts/1. MSA Gateware (Enteron) 1_images/page0.jpeg', 'image_extracts/1. MSA Gateware (Enteron) 1_images/page1.jpeg', 'image_extracts/1. MSA Gateware (Enteron) 1_images/page2.jpeg', 'image_extracts/1. MSA Gateware (Enteron) 1_images/page3.jpeg', 'image_extracts/1. MSA Gateware (Enteron) 1_images/page4.jpeg', 'image_extracts/1. MSA Gateware (Enteron) 1_images/page5.jpeg', 'image_extracts/1. MSA Gateware (Enteron) 1_images/page6.jpeg', 'image_extracts/1. MSA Gateware (Enteron) 1_images/page7.jpeg', 'image_extracts/1. MSA Gateware (Enteron) 1_images/page8.jpeg', 'image_extracts/1. MSA Gateware (Enteron) 1_images/page9.jpeg', 'image_extracts/1. MSA Gateware (Enteron) 1_images/page10.jpeg', 'image_extracts/1. MSA Gateware (Enteron) 1_images/page11.jpeg', 'image_extracts/1. MSA Gateware (Enteron) 1_images/page12.jpeg', 'image_extracts/1. MSA Gateware (Enteron) 1_images/page13.jpeg', 'image_extracts/1. MSA Gateware (Enteron) 1_images/page14.jpeg', 'image_extracts/1. MSA Gateware (Enteron) 1_images/page15.jpeg', 'image_extracts/1. MSA Gateware (Enteron) 1_images/page16.jpeg', 'image_extracts/1. MSA Gateware (Enteron) 1_images/page17.jpeg', 'image_extracts/1. MSA Gateware (Enteron) 1_images/page18.jpeg', 'image_extracts/1. MSA Gateware (Enteron) 1_images/page19.jpeg', 'image_extracts/1. MSA Gateware (Enteron) 1_images/page20.jpeg', 'image_extracts/1. MSA Gateware (Enteron) 1_images/page21.jpeg', 'image_extracts/1. MSA Gateware (Enteron) 1_images/page22.jpeg', 'image_extracts/1. MSA Gateware (Enteron) 1_images/page23.jpeg', 'image_extracts/1. MSA Gateware (Enteron) 1_images/page24.jpeg', 'image_extracts/1. MSA Gateware (Enteron) 1_images/page25.jpeg', 'image_extracts/1. MSA Gateware (Enteron) 1_images/page26.jpeg', 'image_extracts/1. MSA Gateware (Enteron) 1_images/page27.jpeg', 'image_extracts/1. MSA Gateware (Enteron) 1_images/page28.jpeg', 'image_extracts/1. MSA Gateware (Enteron) 1_images/page29.jpeg', 'image_extracts/1. MSA Gateware (Enteron) 1_images/page30.jpeg', 'image_extracts/1. MSA Gateware (Enteron) 1_images/page31.jpeg']\n"
     ]
    }
   ],
   "source": [
    "# pdf to images\n",
    "pdf_file = os.listdir(\"data\")[0]\n",
    "pdf_path = os.path.join(os.getcwd(), \"data\", pdf_file)\n",
    "saved_paths = convert_pdf_to_images(pdf_path, \"image_extracts\")\n",
    "print(\"Images saved to:\", saved_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "# adding paths to each image file\n",
    "file_name = pdf_file.replace(\".pdf\", \"\")\n",
    "docs_ls = os.listdir(f\"image_extracts/{file_name}_images\")\n",
    "docs_ls = [os.path.join(f\"image_extracts/{file_name}_images\", doc) for doc in docs_ls]\n",
    "print(len(docs_ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting information from image_extracts/1. MSA Gateware (Enteron) 1_images/page24.jpeg...\n"
     ]
    }
   ],
   "source": [
    "# Running the final pipeline on the first image\n",
    "docs = final_image_extraction_pipeline(\n",
    "    image_path=\"image_extracts/1. MSA Gateware (Enteron) 1_images/page24.jpeg\",\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    model_name=os.getenv(\"CHAT_ENGINE_GPT4_DEPLOYMENT_NAME\"),\n",
    "    api_version=os.getenv(\"OPENAI_API_VERSION\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docs.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline_in_parallel(docs_ls, api_key, endpoint, model_name, api_version):\n",
    "    \"\"\"\n",
    "    Runs the image extraction pipeline in parallel for all images in the list.\n",
    "\n",
    "    Args:\n",
    "    - docs_ls (list): List of image paths.\n",
    "    - api_key (str): API key for authentication.\n",
    "    - endpoint (str): Base URL of the API endpoint.\n",
    "    - model_name (str): Name of the GPT model.\n",
    "    - api_version (str): API version.\n",
    "\n",
    "    Returns:\n",
    "    - list: List of LangChain Document objects containing extracted content and metadata.\n",
    "    \"\"\"\n",
    "    # Create a partial function with the static arguments\n",
    "    pipeline_func = partial(\n",
    "        final_image_extraction_pipeline,\n",
    "        api_key=api_key,\n",
    "        endpoint=endpoint,\n",
    "        model_name=model_name,\n",
    "        api_version=api_version,\n",
    "    )\n",
    "\n",
    "    # Run the function in parallel for all image paths\n",
    "    results = Parallel(n_jobs=4)(\n",
    "        delayed(pipeline_func)(image_path) for image_path in docs_ls\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the final pipeline in parallel\n",
    "documents_ls = run_pipeline_in_parallel(\n",
    "    docs_ls=docs_ls,\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    model_name=os.getenv(\"CHAT_ENGINE_GPT4_DEPLOYMENT_NAME\"),\n",
    "    api_version=os.getenv(\"OPENAI_API_VERSION\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Documents created:\", len(documents_ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"data_extracts\", exist_ok=True)\n",
    "with open(f\"data_extracts/{file_name}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(documents_ls, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes:\n",
    "# - The pipeline is working fine for the images in parallel.\n",
    "# - The extracted content is documented in the LangChain Document objects.\n",
    "# Retry decorator is non-pickeable, so it is not possible to use it in the parallel processing. so used manual retry logic."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_unstructured",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
