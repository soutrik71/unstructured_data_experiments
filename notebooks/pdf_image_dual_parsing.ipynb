{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 300\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\n",
    "    \"/mnt/batch/tasks/shared/LS_root/mounts/clusters/copilot-model-run/code/Users/Soutrik.Chowdhury/unstructured_data_experiments\"\n",
    ")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __GPT4O_Parsing__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    ")\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import base64\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "from langchain.schema import Document\n",
    "import re\n",
    "import shutil\n",
    "from joblib import Parallel, delayed\n",
    "from functools import partial\n",
    "import pickle\n",
    "from IPython.display import Image, display\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(find_dotenv(\"dev.env\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encode_image(image_path):\n",
    "    \"\"\"\n",
    "    Encodes an image to base64 utf-8 string.\n",
    "\n",
    "    Args:\n",
    "    - image_path (str): Path to the image file.\n",
    "\n",
    "    Returns:\n",
    "    - str: Base64 encoded string of the image.\n",
    "    \"\"\"\n",
    "    with open(image_path, \"rb\") as img_file:\n",
    "        encoded_image = base64.b64encode(img_file.read()).decode(\"utf-8\")\n",
    "    return encoded_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpt_model(\n",
    "    azure_deployment,\n",
    "    model_name,\n",
    "    api_key,\n",
    "    azure_endpoint,\n",
    "    openai_api_type,\n",
    "    api_version,\n",
    "    temperature,\n",
    "    request_timeout,\n",
    "    max_retries,\n",
    "    seed,\n",
    "    top_p,\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns an instance of the AzureChatOpenAI class.\n",
    "\n",
    "    Args:\n",
    "    - azure_deployment (str): Azure deployment name.\n",
    "    - model_name (str): Name of the model.\n",
    "    - api_key (str): API key.\n",
    "    - azure_endpoint (str): Azure endpoint.\n",
    "    - openai_api_type (str): OpenAI API type.\n",
    "    - api_version (str): API version.\n",
    "    - temperature (float): Temperature for sampling.\n",
    "    - request_timeout (int): Request timeout.\n",
    "    - max_retries (int): Maximum number of retries.\n",
    "    - seed (int): Seed for random number generator.\n",
    "    - top_p (float): Top-p sampling.\n",
    "\n",
    "    Returns:\n",
    "    - AzureChatOpenAI: Instance of the AzureChatOpenAI class.\n",
    "    \"\"\"\n",
    "\n",
    "    llm_model = llm = AzureChatOpenAI(\n",
    "        azure_deployment=azure_deployment,\n",
    "        model_name=model_name,\n",
    "        api_key=api_key,\n",
    "        azure_endpoint=azure_endpoint,\n",
    "        openai_api_type=openai_api_type,\n",
    "        api_version=api_version,\n",
    "        temperature=temperature,\n",
    "        request_timeout=request_timeout,\n",
    "        max_retries=max_retries,\n",
    "        seed=seed,\n",
    "        top_p=top_p,\n",
    "    )\n",
    "\n",
    "    return llm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(system_message):\n",
    "    \"\"\"\n",
    "    Returns a ChatPromptTemplate instance with an image and system message.\n",
    "\n",
    "    Args:\n",
    "    - system_message (str): System message to display.\n",
    "\n",
    "    Returns:\n",
    "    - ChatPromptTemplate: Instance of the ChatPromptTemplate class.\n",
    "\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_message),\n",
    "            (\n",
    "                \"user\",\n",
    "                [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"Extract all the text from the image while considering most optimal ways to extract text out of tables and figures\",\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": \"data:image/jpeg;base64,{image_data}\"},\n",
    "                    },\n",
    "                ],\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_op_parser(response):\n",
    "    \"\"\"\n",
    "    Returns the output parser for the response.\n",
    "\n",
    "    Args:\n",
    "    - response (str): Response from the model.\n",
    "\n",
    "    Returns:\n",
    "    - JsonOutputParser: Instance of the JsonOutputParser class.\n",
    "    \"\"\"\n",
    "    response_op = response.dict()\n",
    "    pg_content = response_op[\"content\"]\n",
    "    token_size = response_op[\"usage_metadata\"][\"output_tokens\"]\n",
    "\n",
    "    return json.dumps({\"pg_content\": pg_content, \"token_size\": token_size})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageInformation(BaseModel):\n",
    "    \"\"\"Information extracted from the image in the form of free text.\"\"\"\n",
    "\n",
    "    pg_content: str = Field(\n",
    "        description=\"Entire text extracted out of the image by the AI agent\"\n",
    "    )\n",
    "    token_size: int = Field(description=\"Number of tokens recorded for the output text\")\n",
    "\n",
    "\n",
    "final_parser = JsonOutputParser(pydantic_object=ImageInformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def image_to_text_chain(system_message: str, encoded_image: bytes, llm_model):\n",
    "    \"\"\"\n",
    "    Extracts information from an image containing text, tables, and figures.\n",
    "\n",
    "    Args:\n",
    "    - system_message (str): System message to display.\n",
    "    - encoded_image (bytes): Base64 encoded image.\n",
    "    - llm_model (AzureChatOpenAI): Instance of the AzureChatOpenAI class.\n",
    "\n",
    "    Returns:\n",
    "    - ImageInformation: Information extracted from the image in the form of free text.\n",
    "    \"\"\"\n",
    "    prompt = get_prompt(system_message)\n",
    "    chain = prompt | llm_model | RunnableLambda(func=get_op_parser) | final_parser\n",
    "    response_op = await chain.ainvoke({\"image_data\": encoded_image})\n",
    "    return response_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_lc_document_from_response(response, image_path):\n",
    "    \"\"\"\n",
    "    Creates a LangChain Document object from the GPT model's response.\n",
    "\n",
    "    Args:\n",
    "    - response (dict): Response from the GPT model.\n",
    "    - image_path (str): Path to the image file.\n",
    "\n",
    "    Returns:\n",
    "    - Document: LangChain Document object containing extracted content and metadata.\n",
    "    \"\"\"\n",
    "    if \".pdf\" or \".jpeg\" in image_path:\n",
    "        image_path = image_path.replace(\".pdf\", \"\").replace(\".jpeg\", \"\")\n",
    "    curr_time = datetime.now()\n",
    "    whole_pdf_name = re.findall(r\"(.*)_images\\b\", image_path.split(\"/\")[-2])[0]\n",
    "    file_name = whole_pdf_name.split(\"__\")[-1]\n",
    "    file_version = whole_pdf_name.split(\"__\")[-2]\n",
    "    contractor_name = whole_pdf_name.split(\"__\")[-3]\n",
    "    page_num = re.findall(r\"page(\\d+)\", image_path.split(\"/\")[-1])[0]\n",
    "\n",
    "    # langchain document object\n",
    "\n",
    "    doc = Document(\n",
    "        page_content=response[\"pg_content\"],\n",
    "        metadata={\n",
    "            \"source\": whole_pdf_name,\n",
    "            \"file_name\": file_name,\n",
    "            \"file_version\": file_version,\n",
    "            \"entity_name\": contractor_name,\n",
    "            \"page_number\": int(page_num),\n",
    "            \"token_size\": response[\"token_size\"],\n",
    "            \"timestamp\": str(curr_time),\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def final_image_extraction_pipeline(image_path, system_message, llm_model):\n",
    "    \"\"\"\n",
    "    Extracts information from an image containing text, tables, and figures.\n",
    "\n",
    "    Args:\n",
    "    - image_path (str): Path to the image file.\n",
    "    - system_message (str): System message to display.\n",
    "    - llm_model (AzureChatOpenAI): Instance of the AzureChatOpenAI class.\n",
    "\n",
    "    Returns:\n",
    "    - Document: LangChain Document object containing extracted content and metadata\n",
    "    \"\"\"\n",
    "    encoded_image = get_encode_image(image_path)\n",
    "    response_op = await image_to_text_chain(system_message, encoded_image, llm_model)\n",
    "    lc_doc = await get_lc_document_from_response(response_op, image_path)\n",
    "    return lc_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model = get_gpt_model(\n",
    "    azure_deployment=os.getenv(\"CHAT_ENGINE_GPT4_DEPLOYMENT_NAME\"),\n",
    "    model_name=os.getenv(\"CHAT_ENGINE_GPT4_MODEL_NAME\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    openai_api_type=os.getenv(\"OPENAI_API_TYPE\"),\n",
    "    api_version=os.getenv(\"OPENAI_API_VERSION\"),\n",
    "    temperature=0.0,\n",
    "    request_timeout=45,\n",
    "    max_retries=5,\n",
    "    seed=1234,\n",
    "    top_p=0.0001,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = (\n",
    "    \"You are an AI assistant specializing in extracting information from images containing three main components: \"\n",
    "    \"`text`, `tables`, and `figures`. Your tasks are as follows:\\n\"\n",
    "    \"* Extract the `text` exactly as it appears in the image.\\n\"\n",
    "    \"* Extract data from `tables` while preserving the table structure, providing the data in a structured 2D format, \"\n",
    "    \"and maintaining hierarchical columns if present and always ensuring the correct structure.\\n\"\n",
    "    \"* Extract and describe the information from `figures` in plain text format, making it understandable and reproducible \"\n",
    "    \"from the extracted data.\\n\\n\"\n",
    "    \"The final output should be in plain text format, containing only the extracted information from the original document.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single Execution Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read the label tagged pickle file to read the images with needs gpt4O extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = os.listdir(\"image_extracts\")[-4]\n",
    "print(folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f\"image_extracts/{folder_name}\"\n",
    "image_label = pickle.load(open(f\"{path}/images_labels.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4_images = list(\n",
    "    {key: value for key, value in image_label.items() if value == 1}.keys()\n",
    ")\n",
    "print(f\"Total images to be processed by GPT4O: {len(gpt4_images)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_parsing_pages = [\n",
    "    re.findall(\"\\d+\", k.split(\"/\")[-1])[0]\n",
    "    for k in list(\n",
    "        {key: value for key, value in image_label.items() if value == 0}.keys()\n",
    "    )\n",
    "]\n",
    "print(f\"Total images to be processed by regular parsers: {len(normal_parsing_pages)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = gpt4_images[random.randint(0, len(gpt4_images))]\n",
    "print(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview image for context\n",
    "display(Image(image_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_doc = await final_image_extraction_pipeline(image_path, system_message, llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lc_doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lc_doc.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Parallel Code__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the patch to allow nested event loops\n",
    "nest_asyncio.apply()  # only needed for jupyter notebooks remove for production\n",
    "\n",
    "# Define a semaphore with a limit for concurrent tasks\n",
    "semaphore = asyncio.Semaphore(\n",
    "    3\n",
    ")  # Change the number of parallel tasks you want to allow\n",
    "\n",
    "\n",
    "async def process_image_with_semaphore(image_path, system_message, llm_model):\n",
    "    async with semaphore:\n",
    "        return await final_image_extraction_pipeline(\n",
    "            image_path, system_message, llm_model\n",
    "        )\n",
    "\n",
    "\n",
    "async def process_images_parallel(image_paths, system_message, llm_model):\n",
    "    \"\"\"\n",
    "    Processes a list of images in parallel using the final_image_extraction_pipeline function\n",
    "    with a limit on the number of concurrent tasks.\n",
    "\n",
    "    Args:\n",
    "    - image_paths (list): List of image file paths.\n",
    "    - system_message (str): System message to display.\n",
    "    - llm_model (AzureChatOpenAI): Instance of the AzureChatOpenAI class.\n",
    "\n",
    "    Returns:\n",
    "    - list: List of LangChain Document objects containing extracted content and metadata.\n",
    "    \"\"\"\n",
    "    # Create a list of tasks for processing each image\n",
    "    tasks = [\n",
    "        process_image_with_semaphore(image_path, system_message, llm_model)\n",
    "        for image_path in image_paths\n",
    "    ]\n",
    "\n",
    "    # Run all tasks in parallel and gather the results\n",
    "    documents = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_doc_ls = await process_images_parallel(gpt4_images, system_message, llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keeping on doc class\n",
    "lc_doc_ls = [doc for doc in lc_doc_ls if isinstance(doc, Document)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lc_doc_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_doc_ls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(lc_doc_ls[77].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(lc_doc_ls[77].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __RegularParserExtraction__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import (\n",
    "    PyPDFLoader,\n",
    "    PyPDFium2Loader,\n",
    "    PDFPlumberLoader,\n",
    ")\n",
    "import tiktoken\n",
    "import pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(normal_parsing_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_file_path = folder_name.replace(\"__\", \"/\").replace(\"_images\", \".pdf\")\n",
    "data_path = f\"contracts/{org_file_path}\"\n",
    "print(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf = pdfplumber.open(\n",
    "#     \"./contracts/Mouts/Original/MSA- Services MOUTS REVISED - April 16th v7 (2) 3.pdf\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFium2Loader(data_path)\n",
    "docs = loader.load()\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove thise content where page number is not in normal_parsing_pages\n",
    "docs = [doc for doc in docs if str(doc.metadata[\"page\"]) in normal_parsing_pages]\n",
    "print(len(docs))\n",
    "assert len(docs) == len(normal_parsing_pages), \"Number of pages mismatch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate tiktoken token count\n",
    "def num_tokens_from_string(string: str, encoding_name: str = \"cl100k_base\") -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_lc_document_from_response(docs, data_path):\n",
    "    \"\"\"\n",
    "    Creates a LangChain Document object from the LC pdf parser response.\n",
    "\n",
    "    Args:\n",
    "        docs: list of LangChain Document objects containing extracted content and metadata.\n",
    "        data_path: str, path to the pdf file.\n",
    "\n",
    "    Returns:\n",
    "        doc: list of LangChain Document objects containing extracted content and metadata.\n",
    "\n",
    "    \"\"\"\n",
    "    if \".pdf\" or \".jpeg\" in data_path:\n",
    "        data_path = data_path.replace(\".pdf\", \"\").replace(\".jpeg\", \"\")\n",
    "\n",
    "    curr_time = datetime.now()\n",
    "    whole_pdf_name = (\n",
    "        data_path.split(\"/\")[-3]\n",
    "        + \"__\"\n",
    "        + data_path.split(\"/\")[-2]\n",
    "        + \"__\"\n",
    "        + data_path.split(\"/\")[-1]\n",
    "    )\n",
    "    file_name = data_path.split(\"/\")[-1]\n",
    "    entity_name = data_path.split(\"/\")[-3]\n",
    "    file_version = data_path.split(\"/\")[-2]\n",
    "\n",
    "    # langchain document object , restructing the response\n",
    "\n",
    "    doc = [\n",
    "        Document(\n",
    "            page_content=doc.page_content,\n",
    "            metadata={\n",
    "                \"source\": whole_pdf_name,\n",
    "                \"file_name\": file_name,\n",
    "                \"page_number\": doc.metadata[\"page\"],\n",
    "                \"token_size\": num_tokens_from_string(doc.page_content),\n",
    "                \"timestamp\": str(curr_time),\n",
    "                \"entity_name\": entity_name,\n",
    "                \"file_version\": file_version,\n",
    "            },\n",
    "        )\n",
    "        for doc in docs\n",
    "    ]\n",
    "\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_parsed_docs = await get_lc_document_from_response(docs, org_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (\n",
    "    pdf_parsed_docs[0].metadata.keys() == lc_doc_ls[0].metadata.keys()\n",
    "), \"Metadata keys mismatch for both parsers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pdf_parsed_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding up both list of parsed documents and extracted documents from GPT4O and sorting them by page number\n",
    "all_docs = sorted(pdf_parsed_docs + lc_doc_ls, key=lambda x: x.metadata[\"page_number\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f\"extracted_docs/{folder_name}\", exist_ok=True)\n",
    "pickle.dump(\n",
    "    all_docs, open(f\"extracted_docs/{folder_name}/extracted_docs_all_parsed.pkl\", \"wb\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##################################################################################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_unstructured",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
