{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 300\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\n",
    "    \"/mnt/batch/tasks/shared/LS_root/mounts/clusters/copilot-model-run/code/Users/Soutrik.Chowdhury/unstructured_data_experiments\"\n",
    ")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pdf2image import convert_from_path\n",
    "# import os\n",
    "# from dotenv import load_dotenv, find_dotenv\n",
    "# import base64\n",
    "# from IPython.display import Image, display\n",
    "# import shutil\n",
    "# from PIL import Image as PILImage\n",
    "# from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# from torchvision import transforms\n",
    "# import torch\n",
    "# import warnings\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.patches as patches\n",
    "# from matplotlib.patches import Patch\n",
    "# from transformers import AutoModelForObjectDetection, AutoImageProcessor\n",
    "# from transformers import DetrImageProcessor\n",
    "# from transformers import DetrForObjectDetection\n",
    "# import random\n",
    "# import pytesseract\n",
    "# from pytesseract import Output\n",
    "\n",
    "# warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_dotenv(find_dotenv(\"dev.env\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def detect_and_remove_footer(image, threshold=200):\n",
    "#     \"\"\"\n",
    "#     Detect and remove the footer containing page numbers from the image.\n",
    "\n",
    "#     Args:\n",
    "#     - image (PIL.Image): The image to be processed.\n",
    "#     - threshold (int): Pixel intensity threshold to identify the footer.\n",
    "\n",
    "#     Returns:\n",
    "#     - PIL.Image: The image with the footer removed.\n",
    "#     \"\"\"\n",
    "#     width, height = image.size\n",
    "#     # Consider bottom 10% as potential footer\n",
    "#     footer_height = int(height * 0.1)\n",
    "#     footer_image = image.crop((0, height - footer_height, width, height))\n",
    "\n",
    "#     # Use OCR to detect text in the footer region\n",
    "#     ocr_result = pytesseract.image_to_data(\n",
    "#         footer_image, output_type=Output.DICT)\n",
    "\n",
    "#     # Check if there's a page number in the detected text\n",
    "#     footer_contains_number = any(word.isdigit() for word in ocr_result[\"text\"])\n",
    "\n",
    "#     if footer_contains_number:\n",
    "#         # Remove the footer containing the page number\n",
    "#         cropped_image = image.crop((0, 0, width, height - footer_height))\n",
    "#         return cropped_image\n",
    "#     else:\n",
    "#         # No page number detected, return the original image\n",
    "#         return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_pdf_to_images(\n",
    "#     pdf_path, dest_folder, image_format=\"JPEG\", remove_footer=True\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Converts each page of a PDF into images and saves them in a directory.\n",
    "\n",
    "#     Args:\n",
    "#     - pdf_path (str): Path to the PDF file.\n",
    "#     - dest_folder (str): Destination folder to save the images.\n",
    "#     - image_format (str): Format to save the images (default is \"JPEG\").\n",
    "\n",
    "#     Returns:\n",
    "#     - list: List of image file paths saved.\n",
    "#     \"\"\"\n",
    "\n",
    "#     file_name = pdf_path.split(\"/\")[-1].split(\".\")[0]\n",
    "#     file_version = pdf_path.split(\"/\")[-2]\n",
    "#     contractor_name = pdf_path.split(\"/\")[-3]\n",
    "\n",
    "#     pdf_name = f\"{contractor_name}__{file_version}__{file_name}\"\n",
    "#     output_dir = os.path.join(dest_folder, f\"{pdf_name}_images\")\n",
    "#     if os.path.exists(output_dir):\n",
    "#         # remove the existing directory\n",
    "#         shutil.rmtree(output_dir)\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "#     images = convert_from_path(pdf_path)\n",
    "#     saved_image_paths = []\n",
    "\n",
    "#     for i, img in enumerate(images):\n",
    "#         if remove_footer:\n",
    "#             img = detect_and_remove_footer(img)\n",
    "#         image_path = os.path.join(\n",
    "#             output_dir, f\"page{int(i+1)}.{image_format.lower()}\")\n",
    "#         img.save(image_path, image_format)\n",
    "#         saved_image_paths.append(image_path)\n",
    "\n",
    "#     return saved_image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root_folder = os.path.join(os.getcwd(), \"contracts/Accenture\")\n",
    "# pdf_files = glob(os.path.join(root_folder, \"**\", \"*.pdf\"), recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_a = pdf_files[0]\n",
    "# print(file_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saved_paths = convert_pdf_to_images(file_a, \"image_extracts\")\n",
    "# print(\"Images saved to:\", saved_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # adding paths to each image file\n",
    "# img_folder_names = os.listdir(\"image_extracts\")\n",
    "# print(img_folder_names)\n",
    "# file_name = img_folder_names[-2]\n",
    "# docs_ls = os.listdir(f\"image_extracts/{file_name}\")\n",
    "# docs_ls = [os.path.join(f\"image_extracts/{file_name}\", doc) for doc in docs_ls]\n",
    "# print(len(docs_ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"cpu\"\n",
    "# detection_model_name = \"microsoft/table-transformer-detection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def read_images(file_path, plot=True):\n",
    "#     \"\"\"Read image from file path and display it.\"\"\"\n",
    "#     image = Image.open(file_path).convert(\"RGB\")\n",
    "#     if plot:\n",
    "#         width, height = image.size\n",
    "#         fig, ax = plt.subplots(1, figsize=(12, 12))\n",
    "#         img = image.resize((int(width * 0.5), int(height * 0.5)))\n",
    "#         ax.imshow(img)\n",
    "#         plt.axis(\"off\")\n",
    "#         plt.show()\n",
    "\n",
    "#         return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_file_path = docs_ls[random.randint(0, len(docs_ls) - 1)]\n",
    "# print(sample_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = read_images(sample_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_model(detection_model_name, device):\n",
    "#     \"\"\"Load the model and image processor.\"\"\"\n",
    "#     image_processor = AutoImageProcessor.from_pretrained(\n",
    "#         detection_model_name, force_download=True\n",
    "#     )\n",
    "#     model = AutoModelForObjectDetection.from_pretrained(\n",
    "#         detection_model_name, revision=\"no_timm\"\n",
    "#     )\n",
    "#     model.to(device)\n",
    "\n",
    "#     return model, image_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detection_model, image_processor = get_model(detection_model_name, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # update id2label to include \"no object\"\n",
    "# id2label = detection_model.config.id2label\n",
    "# id2label[len(detection_model.config.id2label)] = \"no object\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def model_prediction(image, detection_model, image_processor):\n",
    "#     \"\"\"Get model predictions.\"\"\"\n",
    "#     pixel_values_img = image_processor(image, return_tensors=\"pt\")\n",
    "#     with torch.no_grad():\n",
    "#         outputs = detection_model(**pixel_values_img)\n",
    "\n",
    "#     return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs = model_prediction(image, detection_model, image_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for output bounding box post-processing\n",
    "# def box_cxcywh_to_xyxy(x):\n",
    "#     \"\"\"Convert bounding boxes from [x, y, w, h] format to [x1, y1, x2, y2] format.\"\"\"\n",
    "#     x_c, y_c, w, h = x.unbind(-1)\n",
    "#     b = [(x_c - 0.5 * w), (y_c - 0.5 * h), (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
    "#     return torch.stack(b, dim=1)\n",
    "\n",
    "\n",
    "# def rescale_bboxes(out_bbox, size):\n",
    "#     \"\"\"Rescale bounding boxes from [0, 1] to image size.\"\"\"\n",
    "#     img_w, img_h = size\n",
    "#     b = box_cxcywh_to_xyxy(out_bbox)\n",
    "#     b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n",
    "#     return b\n",
    "\n",
    "\n",
    "# def outputs_to_objects(outputs, img_size, id2label):\n",
    "#     \"\"\"Convert model outputs to objects list\"\"\"\n",
    "#     m = outputs.logits.softmax(-1).max(-1)\n",
    "#     pred_labels = list(m.indices.detach().cpu().numpy())[0]\n",
    "#     pred_scores = list(m.values.detach().cpu().numpy())[0]\n",
    "#     pred_bboxes = outputs[\"pred_boxes\"].detach().cpu()[0]\n",
    "#     pred_bboxes = [elem.tolist()\n",
    "#                    for elem in rescale_bboxes(pred_bboxes, img_size)]\n",
    "\n",
    "#     objects = []\n",
    "#     for label, score, bbox in zip(pred_labels, pred_scores, pred_bboxes):\n",
    "#         class_label = id2label[int(label)]\n",
    "#         if not class_label == \"no object\":\n",
    "#             objects.append(\n",
    "#                 {\n",
    "#                     \"label\": class_label,\n",
    "#                     \"score\": float(score),\n",
    "#                     \"bbox\": [float(elem) for elem in bbox],\n",
    "#                 }\n",
    "#             )\n",
    "\n",
    "#     return objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# op_objects = outputs_to_objects(outputs, image.size, id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def target_post_processing(image, outputs, threshold):\n",
    "#     \"\"\"Post-process the target objects.\"\"\"\n",
    "#     target_sizes = torch.tensor([image.size[::-1]])\n",
    "#     results = image_processor.post_process_object_detection(\n",
    "#         outputs, threshold=threshold, target_sizes=target_sizes\n",
    "#     )[0]\n",
    "#     return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = target_post_processing(image, outputs, threshold=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_table_bbox(results, detection_model):\n",
    "#     \"\"\"Get table bounding box coordinates.\"\"\"\n",
    "#     tables_coordinates = []\n",
    "\n",
    "#     # iterate through all the detected table data\n",
    "#     for score, label, box in zip(\n",
    "#         results[\"scores\"], results[\"labels\"], results[\"boxes\"]\n",
    "#     ):\n",
    "#         # Convert tensor to list and then round the elements\n",
    "#         box = [round(i.item(), 2) for i in box]\n",
    "\n",
    "#         # store bbox coodinates in Pascal VOC format for later use\n",
    "#         table_dict = {\"xmin\": box[0], \"ymin\": box[1],\n",
    "#                       \"xmax\": box[2], \"ymax\": box[3]}\n",
    "\n",
    "#         tables_coordinates.append(table_dict)\n",
    "\n",
    "#         # print prediction label, prediction confidence score, and bbox values\n",
    "#         print(\n",
    "#             f\"Detected {detection_model.config.id2label[label.item()]} with confidence \"\n",
    "#             f\"{round(score.item(), 3)} at location {box}\"\n",
    "#         )\n",
    "\n",
    "#     return tables_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tables_coordinates = get_table_bbox(results, detection_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def image_table_detection(\n",
    "#     sample_file_path, detection_model, image_processor, threshold=0.8\n",
    "# ):\n",
    "#     \"\"\"Get table detection results.\"\"\"\n",
    "#     image = read_images(sample_file_path)\n",
    "#     outputs = model_prediction(image, detection_model, image_processor)\n",
    "#     results = target_post_processing(image, outputs, threshold)\n",
    "#     tables_coordinates = get_table_bbox(results, detection_model)\n",
    "#     page_name = sample_file_path.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "#     op_dict = {page_name: 1 if len(tables_coordinates) > 0 else 0}\n",
    "#     return op_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# op_dict = image_table_detection(\n",
    "#     sample_file_path, detection_model, image_processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizing the code for batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import warnings\n",
    "from glob import glob\n",
    "from PIL import Image as PILImage\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModelForObjectDetection, AutoImageProcessor\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "from PIL import ImageOps\n",
    "import numpy as np\n",
    "import pytesseract\n",
    "from pytesseract import Output\n",
    "import pickle\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv(find_dotenv(\"dev.env\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_remove_footer(image, threshold=200):\n",
    "    \"\"\"\n",
    "    Detect and remove the footer containing page numbers from the image.\n",
    "\n",
    "    Args:\n",
    "    - image (PIL.Image): The image to be processed.\n",
    "    - threshold (int): Pixel intensity threshold to identify the footer.\n",
    "\n",
    "    Returns:\n",
    "    - PIL.Image: The image with the footer removed.\n",
    "    \"\"\"\n",
    "    width, height = image.size\n",
    "    # Consider bottom 10% as potential footer\n",
    "    footer_height = int(height * 0.05)\n",
    "    footer_image = image.crop((0, height - footer_height, width, height))\n",
    "\n",
    "    # Use OCR to detect text in the footer region\n",
    "    ocr_result = pytesseract.image_to_data(footer_image, output_type=Output.DICT)\n",
    "\n",
    "    # Check if there's a page number in the detected text\n",
    "    footer_contains_number = any(word.isalpha() for word in ocr_result[\"text\"])\n",
    "\n",
    "    if footer_contains_number:\n",
    "        # Remove the footer containing the page number\n",
    "        cropped_image = image.crop((0, 0, width, height - footer_height))\n",
    "        return cropped_image\n",
    "    else:\n",
    "        # No page number detected, return the original image\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pdf_to_images(\n",
    "    pdf_path, dest_folder, image_format=\"JPEG\", remove_footer=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Converts each page of a PDF into images and saves them in a directory.\n",
    "\n",
    "    Args:\n",
    "    - pdf_path (str): Path to the PDF file.\n",
    "    - dest_folder (str): Destination folder to save the images.\n",
    "    - image_format (str): Format to save the images (default is \"JPEG\").\n",
    "\n",
    "    Returns:\n",
    "    - list: List of image file paths saved.\n",
    "    \"\"\"\n",
    "\n",
    "    file_name = pdf_path.split(\"/\")[-1].split(\".\")[0]\n",
    "    file_version = pdf_path.split(\"/\")[-2]\n",
    "    main_entity = pdf_path.split(\"/\")[-3]\n",
    "\n",
    "    pdf_name = f\"{main_entity}__{file_version}__{file_name}\"\n",
    "    output_dir = os.path.join(dest_folder, f\"{pdf_name}_images\")\n",
    "    if os.path.exists(output_dir):\n",
    "        # remove the existing directory\n",
    "        shutil.rmtree(output_dir)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    images = convert_from_path(pdf_path)\n",
    "    saved_image_paths = []\n",
    "\n",
    "    for i, img in enumerate(images):\n",
    "        if remove_footer:\n",
    "            img = detect_and_remove_footer(img)\n",
    "        image_path = os.path.join(output_dir, f\"page{int(i+1)}.{image_format.lower()}\")\n",
    "        img.save(image_path, image_format)\n",
    "        saved_image_paths.append(image_path)\n",
    "\n",
    "    return saved_image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = \"contracts/Enteron\"\n",
    "root_folder = os.path.join(os.getcwd(), folder_name)\n",
    "pdf_files = glob(os.path.join(root_folder, \"**\", \"*.pdf\"), recursive=True)\n",
    "print(pdf_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_a = pdf_files[-1]\n",
    "print(file_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_paths = convert_pdf_to_images(file_a, \"image_extracts\")\n",
    "print(\"Images saved to:\", saved_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_whitespaces(image, padding=5, plot=False):\n",
    "    \"\"\"\n",
    "    Crop the image to remove whitespace around the content.\n",
    "\n",
    "    Args:\n",
    "    - file_path (str): Path to the image file.\n",
    "    - padding (int): Padding around the content to keep.\n",
    "\n",
    "    Returns:\n",
    "    - PIL.Image: Cropped image.\n",
    "    \"\"\"\n",
    "    padding = 5\n",
    "    padding = np.asarray([-1 * padding, -1 * padding, padding, padding])\n",
    "\n",
    "    # invert image (so that white is 0)\n",
    "    invert_im = ImageOps.invert(image)\n",
    "    imageBox = invert_im.getbbox()\n",
    "    imageBox = tuple(np.asarray(imageBox) + padding)\n",
    "\n",
    "    cropped_image = image.crop(imageBox)\n",
    "\n",
    "    if plot:\n",
    "        width, height = cropped_image.size\n",
    "        fig, ax = plt.subplots(1, figsize=(12, 12))\n",
    "        plt.title(\"Cropped Image\")\n",
    "        img = cropped_image.resize((int(width * 0.5), int(height * 0.5)))\n",
    "        ax.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "    return cropped_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_images(file_path, plot=True, crop_whites=True):\n",
    "    \"\"\"Read image from file path and optionally display it.\"\"\"\n",
    "    image = Image.open(file_path).convert(\"RGB\")\n",
    "    if plot:\n",
    "        img_resized = image.resize((int(image.width * 0.5), int(image.height * 0.5)))\n",
    "        plt.figure(figsize=(12, 12))\n",
    "        plt.title(\"Original Image\")\n",
    "        plt.imshow(img_resized)\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "    if crop_whites:\n",
    "        image = crop_whitespaces(image, plot=plot)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(detection_model_name, device):\n",
    "    \"\"\"Load the model and image processor.\"\"\"\n",
    "    image_processor = AutoImageProcessor.from_pretrained(\n",
    "        detection_model_name, force_download=True\n",
    "    )\n",
    "    model = AutoModelForObjectDetection.from_pretrained(\n",
    "        detection_model_name, revision=\"no_timm\"\n",
    "    ).to(device)\n",
    "    return model, image_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_prediction(image, detection_model, image_processor):\n",
    "    \"\"\"Get model predictions.\"\"\"\n",
    "    pixel_values_img = image_processor(image, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = detection_model(**pixel_values_img)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_cxcywh_to_xyxy(x):\n",
    "    \"\"\"Convert bounding boxes from [x, y, w, h] format to [x1, y1, x2, y2] format.\"\"\"\n",
    "    x_c, y_c, w, h = x.unbind(-1)\n",
    "    return torch.stack(\n",
    "        [(x_c - 0.5 * w), (y_c - 0.5 * h), (x_c + 0.5 * w), (y_c + 0.5 * h)], dim=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_bboxes(out_bbox, size):\n",
    "    \"\"\"Rescale bounding boxes from [0, 1] to image size.\"\"\"\n",
    "    img_w, img_h = size\n",
    "    return box_cxcywh_to_xyxy(out_bbox) * torch.tensor(\n",
    "        [img_w, img_h, img_w, img_h], dtype=torch.float32\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outputs_to_objects(outputs, img_size, id2label):\n",
    "    \"\"\"Convert model outputs to objects list.\"\"\"\n",
    "    m = outputs.logits.softmax(-1).max(-1)\n",
    "    pred_labels = m.indices[0].cpu().numpy()\n",
    "    pred_scores = m.values[0].cpu().numpy()\n",
    "    pred_bboxes = rescale_bboxes(outputs[\"pred_boxes\"][0].cpu(), img_size)\n",
    "\n",
    "    return [\n",
    "        {\"label\": id2label[int(label)], \"score\": float(score), \"bbox\": bbox.tolist()}\n",
    "        for label, score, bbox in zip(pred_labels, pred_scores, pred_bboxes)\n",
    "        if id2label[int(label)] != \"no object\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_post_processing(image, outputs, threshold, image_processor):\n",
    "    \"\"\"Post-process the target objects.\"\"\"\n",
    "    target_sizes = torch.tensor([image.size[::-1]])\n",
    "    return image_processor.post_process_object_detection(\n",
    "        outputs, threshold=threshold, target_sizes=target_sizes\n",
    "    )[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_non_white_pixels(image, bbox):\n",
    "    \"\"\"Check if a bounding box region contains non-white pixels.\"\"\"\n",
    "    xmin, ymin, xmax, ymax = map(int, bbox)\n",
    "    cropped_img = image.crop((xmin, ymin, xmax, ymax))\n",
    "    pixels = cropped_img.getdata()\n",
    "    non_white_pixels = any(pixel != (255, 255, 255) for pixel in pixels)\n",
    "\n",
    "    return non_white_pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_bbox(image, results, detection_model):\n",
    "    \"\"\"Get table bounding box coordinates.\"\"\"\n",
    "    tables_coordinates = []\n",
    "\n",
    "    for score, label, box in zip(\n",
    "        results[\"scores\"], results[\"labels\"], results[\"boxes\"]\n",
    "    ):\n",
    "        box = [round(i.item(), 2) for i in box]\n",
    "        if contains_non_white_pixels(image, box):\n",
    "            table_dict = {\n",
    "                \"xmin\": box[0],\n",
    "                \"ymin\": box[1],\n",
    "                \"xmax\": box[2],\n",
    "                \"ymax\": box[3],\n",
    "            }\n",
    "            tables_coordinates.append(table_dict)\n",
    "            print(\n",
    "                f\"Detected {detection_model.config.id2label[label.item()]} with confidence \"\n",
    "                f\"{round(score.item(), 3)} at location {box}\"\n",
    "            )\n",
    "\n",
    "    return tables_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_table_detection(\n",
    "    sample_file_path, detection_model, image_processor, threshold=0.85\n",
    "):\n",
    "    \"\"\"Get table detection results.\"\"\"\n",
    "    image = read_images(sample_file_path)\n",
    "    outputs = model_prediction(image, detection_model, image_processor)\n",
    "    results = target_post_processing(image, outputs, threshold, image_processor)\n",
    "    print(f\"Detected labels {results['labels']} objects.\")\n",
    "    print(f\"Detected scores{results['scores']} objects.\")\n",
    "    tables_coordinates = get_table_bbox(image, results, detection_model)\n",
    "    return {sample_file_path: int(bool(tables_coordinates))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script execution\n",
    "folder_name = \"contracts/Enteron\"\n",
    "root_folder = os.path.join(os.getcwd(), folder_name)\n",
    "pdf_files = glob(os.path.join(root_folder, \"**\", \"*.pdf\"), recursive=True)\n",
    "file_a = pdf_files[0]\n",
    "print(file_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_folder_names = os.listdir(\"image_extracts\")\n",
    "file_name = img_folder_names[-4]\n",
    "print(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_ls = [\n",
    "    os.path.join(\"image_extracts\", file_name, doc)\n",
    "    for doc in os.listdir(f\"image_extracts/{file_name}\")\n",
    "]\n",
    "print(len(docs_ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"cpu\"\n",
    "detection_model_name = \"microsoft/table-transformer-detection\"\n",
    "detection_model, image_processor = get_model(detection_model_name, device)\n",
    "model_threshold = 0.90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_file_path = random.choice(docs_ls)\n",
    "print(sample_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_dict = image_table_detection(\n",
    "    sample_file_path, detection_model, image_processor, threshold=model_threshold\n",
    ")\n",
    "print(op_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_image_table_detection(\n",
    "    all_file_paths, detection_model, image_processor, threshold=0.9, n_jobs=-1\n",
    "):\n",
    "    \"\"\"\n",
    "    Run image_table_detection in parallel for multiple sample file paths.\n",
    "\n",
    "    Args:\n",
    "    - sample_file_paths (list): List of file paths to be processed.\n",
    "    - detection_model: Loaded object detection model.\n",
    "    - image_processor: Image processor corresponding to the model.\n",
    "    - threshold (float): Confidence threshold for post-processing.\n",
    "    - n_jobs (int): Number of jobs to run in parallel (default is -1 for all cores).\n",
    "\n",
    "    Returns:\n",
    "    - list: List of dictionaries with detection results for each file path.\n",
    "    \"\"\"\n",
    "    results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(image_table_detection)(\n",
    "            file_path, detection_model, image_processor, threshold\n",
    "        )\n",
    "        for file_path in all_file_paths\n",
    "    )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "op_results = parallel_image_table_detection(\n",
    "    docs_ls, detection_model, image_processor, threshold=model_threshold, n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dict = {}\n",
    "for result in op_results:\n",
    "    all_dict.update(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: For exclusive pdfs where we have scanned images and no text, in that case we can mark all the pages as 1 as normal parser wont be able to extract any text\n",
    "# TODO: If there is any easy ocr that can help specifically for scanned images, we can use that to extract text and then use the normal parser to extract tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = docs_ls[0].split(\"/\")[-2]\n",
    "print(folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    f\"./image_extracts/{folder_name}/images_labels.pkl\",\n",
    "    \"wb\",\n",
    ") as f:\n",
    "    pickle.dump(all_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "* https://huggingface.co/docs/transformers/main/en/model_doc/table-transformer\n",
    "* https://gist.github.com/thomastweets/c7680e41ed88452d3c63401bb35116ed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##################################################################################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_unstructured",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
