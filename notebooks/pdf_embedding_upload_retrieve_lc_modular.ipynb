{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 300\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\n",
    "    \"/mnt/batch/tasks/shared/LS_root/mounts/clusters/copilot-model-run/code/Users/Soutrik.Chowdhury/unstructured_data_experiments\"\n",
    ")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import re\n",
    "import urllib.parse\n",
    "\n",
    "import numpy as np\n",
    "from joblib import delayed, Parallel, parallel_backend\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "from functools import partial\n",
    "from tenacity import retry, stop_after_attempt\n",
    "from typing import Any, Dict, List, Union\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()  # Fixing asyncio bug with Jupyter Notebook\n",
    "\n",
    "from redisvl.index import SearchIndex\n",
    "from redis import Redis\n",
    "from urllib.parse import quote\n",
    "from redisvl.query import VectorQuery\n",
    "from redisvl.query.filter import Tag\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from redisvl.index import AsyncSearchIndex\n",
    "from redis.asyncio import Redis\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import PyPDFium2Loader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "import pickle\n",
    "from redisvl.utils.rerank import HFCrossEncoderReranker\n",
    "from langchain_core.vectorstores import VectorStore, VectorStoreRetriever\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_file = \"dev.env\"\n",
    "\n",
    "load_dotenv(find_dotenv(env_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenAIEmbeddingFunctions:\n",
    "    \"\"\"\n",
    "    Class to get the OpenAIEmbeddings for embedding documents.\n",
    "    Attributes:\n",
    "        api_key (str): The API key of the OpenAI model.\n",
    "        api_base (str): The API base of the OpenAI model.\n",
    "        api_type (str): The API type of the OpenAI model.\n",
    "        api_version (str): The API version of the OpenAI model.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        api_key: str = os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "        api_base: str = os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "        api_type: str = os.environ.get(\"OPENAI_API_TYPE\"),\n",
    "        api_version: str = os.environ.get(\"OPENAI_API_VERSION\"),\n",
    "        model_name: str = os.environ.get(\"EMBEDDING_ENGINE_ADA_DEPLOYMENT_NAME\"),\n",
    "        model_deployment_name: str = os.environ.get(\"EMBEDDING_ENGINE_ADA_MODEL_NAME\"),\n",
    "    ) -> None:\n",
    "        self.api_key = api_key\n",
    "        self.api_base = api_base\n",
    "        self.api_type = api_type\n",
    "        self.api_version = api_version\n",
    "        self.model_name = model_name\n",
    "        self.model_deployment_name = model_deployment_name\n",
    "\n",
    "    def get_openai_embedder(self):\n",
    "        \"\"\"\n",
    "        Get an instance of OpenAIEmbeddings for embedding documents.\n",
    "\n",
    "        Args:\n",
    "            openai_pkg: The OpenAI package.\n",
    "            model (str, optional): The model name. Defaults to None.\n",
    "            deployment (str, optional): The deployment name. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            An instance of OpenAIEmbeddings for embedding documents.\n",
    "        \"\"\"\n",
    "\n",
    "        return AzureOpenAIEmbeddings(\n",
    "            model=self.model_name,\n",
    "            azure_deployment=self.model_deployment_name,\n",
    "            api_key=self.api_key,\n",
    "            azure_endpoint=self.api_base,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpt_model(\n",
    "    azure_deployment,\n",
    "    model_name,\n",
    "    api_key,\n",
    "    azure_endpoint,\n",
    "    openai_api_type,\n",
    "    api_version,\n",
    "    temperature,\n",
    "    request_timeout,\n",
    "    max_retries,\n",
    "    seed,\n",
    "    top_p,\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns an instance of the AzureChatOpenAI class.\n",
    "\n",
    "    Args:\n",
    "    - azure_deployment (str): Azure deployment name.\n",
    "    - model_name (str): Name of the model.\n",
    "    - api_key (str): API key.\n",
    "    - azure_endpoint (str): Azure endpoint.\n",
    "    - openai_api_type (str): OpenAI API type.\n",
    "    - api_version (str): API version.\n",
    "    - temperature (float): Temperature for sampling.\n",
    "    - request_timeout (int): Request timeout.\n",
    "    - max_retries (int): Maximum number of retries.\n",
    "    - seed (int): Seed for random number generator.\n",
    "    - top_p (float): Top-p sampling.\n",
    "\n",
    "    Returns:\n",
    "    - AzureChatOpenAI: Instance of the AzureChatOpenAI class.\n",
    "    \"\"\"\n",
    "\n",
    "    llm_model = llm = AzureChatOpenAI(\n",
    "        azure_deployment=azure_deployment,\n",
    "        model_name=model_name,\n",
    "        api_key=api_key,\n",
    "        azure_endpoint=azure_endpoint,\n",
    "        openai_api_type=openai_api_type,\n",
    "        api_version=api_version,\n",
    "        temperature=temperature,\n",
    "        request_timeout=request_timeout,\n",
    "        max_retries=max_retries,\n",
    "        seed=seed,\n",
    "        top_p=top_p,\n",
    "    )\n",
    "\n",
    "    return llm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read documents from Documents folder\n",
    "class DynamicDocumentSplitter:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        doc_folder_path: str,\n",
    "        split_type: str,\n",
    "        min_word_count: int,\n",
    "        overlap_fraction: float,\n",
    "        max_word_count: int,\n",
    "        documents: list = None,\n",
    "    ):\n",
    "\n",
    "        self.doc_folder_path = doc_folder_path\n",
    "        self.split_type = split_type\n",
    "        self.min_word_count = min_word_count\n",
    "        self.overlap_fraction = overlap_fraction\n",
    "        self.max_word_count = max_word_count\n",
    "        self.documents = documents\n",
    "\n",
    "    def get_document(self):\n",
    "        \"\"\"Get document from the file.\"\"\"\n",
    "        if self.documents:\n",
    "            return self.documents\n",
    "        docs = pickle.load(open(self.doc_folder_path, \"rb\"))\n",
    "        docs = [doc for doc in docs if isinstance(doc, Document)]\n",
    "        return docs\n",
    "\n",
    "    def get_dynamic_chunk_details(self, doc):\n",
    "        \"\"\"Determine dynamic chunk size and overlap for document splitting.\"\"\"\n",
    "        print(\"Calculating chunk details.\")\n",
    "\n",
    "        def count_func(x):\n",
    "            return len(re.findall(r\"\\w+\", x))\n",
    "\n",
    "        dynamic_chunk_sz = int(\n",
    "            np.mean([count_func(doc_element.page_content) for doc_element in doc])\n",
    "        )\n",
    "\n",
    "        if dynamic_chunk_sz < self.min_word_count:\n",
    "            return {\n",
    "                \"chunk_size\": self.min_word_count,\n",
    "                \"chunk_overlap\": int(self.min_word_count * self.overlap_fraction),\n",
    "            }\n",
    "        elif dynamic_chunk_sz > self.max_word_count:\n",
    "            return {\n",
    "                \"chunk_size\": self.max_word_count,\n",
    "                \"chunk_overlap\": int(self.max_word_count * self.overlap_fraction),\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"chunk_size\": dynamic_chunk_sz,\n",
    "                \"chunk_overlap\": int(dynamic_chunk_sz * self.overlap_fraction),\n",
    "            }\n",
    "\n",
    "    def create_chunks_docs(self, docs, chunk_details):\n",
    "        \"\"\"Create document chunks based on the specified splitting method.\"\"\"\n",
    "        print(\"Creating document chunks.\")\n",
    "\n",
    "        text_splitter = (\n",
    "            RecursiveCharacterTextSplitter\n",
    "            if self.split_type == \"recursive\"\n",
    "            else CharacterTextSplitter if self.split_type == \"character\" else None\n",
    "        )\n",
    "        if text_splitter is None:\n",
    "            raise ValueError(\"Invalid split type.\")\n",
    "\n",
    "        docs = text_splitter(\n",
    "            chunk_size=chunk_details[\"chunk_size\"],\n",
    "            chunk_overlap=chunk_details[\"chunk_overlap\"],\n",
    "            add_start_index=True,\n",
    "        ).split_documents(docs)\n",
    "\n",
    "        return docs\n",
    "\n",
    "    def get_chunked_docs(\n",
    "        self,\n",
    "    ):\n",
    "        \"\"\"Get chunks for a given file.\"\"\"\n",
    "        org_docs = self.get_document()\n",
    "        if self.split_type == \"pages\":\n",
    "            return org_docs\n",
    "\n",
    "        chunk_details = self.get_dynamic_chunk_details(org_docs)\n",
    "        chunk_docs = self.create_chunks_docs(org_docs, chunk_details)\n",
    "        return chunk_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_folder_path = \"extracted_docs/Levva__Original__CW194187 -  Levva MSA - Fully Executed 4_images/extracted_docs_all_parsed.pkl\"\n",
    "split_type = \"pages\"\n",
    "min_word_count = 1000\n",
    "overlap_fraction = 0.1\n",
    "max_word_count = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_splitter = DynamicDocumentSplitter(\n",
    "    doc_folder_path, split_type, min_word_count, overlap_fraction, max_word_count\n",
    ")\n",
    "chunks_docs = doc_splitter.get_chunked_docs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(chunks_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Args for the Vector DB Model\n",
    "added_metadata = [\"entity_name\", \"file_version\"]\n",
    "static_metadata = [\n",
    "    \"source\",\n",
    "    \"page_number\",\n",
    "    \"file_name\",\n",
    "    \"token_size\",\n",
    "    \"timestamp\",\n",
    "]\n",
    "open_ai_embedder = OpenAIEmbeddingFunctions().get_openai_embedder()\n",
    "vector_field_name = \"embeddings\"\n",
    "embedding_field_name = \"page_content\"\n",
    "embedding_dimension = 1536\n",
    "distance_metric = \"cosine\"\n",
    "vector_algo = \"hnsw\"\n",
    "drop_index = True\n",
    "llm_re_ranking = False\n",
    "redis_host = os.environ.get(\"REDIS_HOST\")\n",
    "redis_port = os.environ.get(\"REDIS_PORT\")\n",
    "redis_database = os.environ.get(\"REDIS_DATABASE\")\n",
    "redis_password = os.environ.get(\"REDIS_PASSWORD\")\n",
    "index_name = os.environ.get(\"REDIS_INDEX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_to_hash(\n",
    "    all_docs: list, added_metadata: list, static_metadata: list\n",
    ") -> list:\n",
    "    \"\"\"Convert the documents to a hash for indexing as expected by the Vector DB model\"\"\"\n",
    "    hash_docs = [\n",
    "        {\n",
    "            \"page_content\": doc.page_content,\n",
    "        }\n",
    "        | {f\"{meta}\": str(doc.metadata[f\"{meta}\"]).lower() for meta in static_metadata}\n",
    "        | {f\"{meta}\": doc.metadata[f\"{meta}\"].lower() for meta in added_metadata}\n",
    "        for doc in all_docs\n",
    "    ]\n",
    "    return hash_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentEmbedder:\n",
    "    def __init__(self, open_ai_embedder, max_concurrent_tasks: int = 10):\n",
    "        self.open_ai_embedder = open_ai_embedder\n",
    "        self.max_concurrent_tasks = max_concurrent_tasks\n",
    "\n",
    "    async def content_embedder(\n",
    "        self,\n",
    "        content: str,\n",
    "        op_type: str = \"bytes\",\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Embed the content using the OpenAI embeddings to create embeddings for one unit.\n",
    "\n",
    "        Args:\n",
    "            content (str): The content to embed.\n",
    "            op_type (str): The operation type, either 'bytes' or the original array.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The embedding vector.\n",
    "        \"\"\"\n",
    "        embedding_vector = await self.open_ai_embedder.aembed_documents([content])\n",
    "        embd_array = np.array(embedding_vector[0]).astype(np.float32)\n",
    "\n",
    "        if op_type == \"bytes\":\n",
    "            return embd_array.tobytes()\n",
    "\n",
    "        return embd_array\n",
    "\n",
    "    @retry(stop=stop_after_attempt(5))\n",
    "    async def atomic_embedder(\n",
    "        self,\n",
    "        content_dict: dict,\n",
    "        vector_field_name: str,\n",
    "        embedding_field_name: str,\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Embed the content using the OpenAI embeddings to create embeddings for one unit.\n",
    "\n",
    "        Args:\n",
    "            content_dict (dict): The content dictionary.\n",
    "            vector_field_name (str): The name of the vector field.\n",
    "            embedding_field_name (str): The name of the embedding field in the document.\n",
    "\n",
    "        Returns:\n",
    "            dict: The updated content dictionary with the embedding.\n",
    "        \"\"\"\n",
    "        if not isinstance(content_dict, dict):\n",
    "            logger.error(f\"Content is not a dictionary, but {type(content_dict)}\")\n",
    "            raise TypeError(f\"Content is not a dictionary, but {type(content_dict)}\")\n",
    "\n",
    "        item_content = content_dict[embedding_field_name]\n",
    "        embedding_vector = await self.content_embedder(item_content)\n",
    "        content_dict[vector_field_name] = embedding_vector\n",
    "        return content_dict\n",
    "\n",
    "    async def full_embedder(\n",
    "        self,\n",
    "        content_dict: list,\n",
    "        vector_field_name: str,\n",
    "        embedding_field_name: str,\n",
    "    ) -> list:\n",
    "        \"\"\"\n",
    "        Atomic loading of each content dictionary with the embedding field.\n",
    "        This method is supposed to change as per different use cases.\n",
    "\n",
    "        Args:\n",
    "            content_dict (list): The list of content dictionaries.\n",
    "            vector_field_name (str): The name of the vector field.\n",
    "            embedding_field_name (str): The name of the embedding field in the document.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of content dictionaries with the embedding field updated.\n",
    "        \"\"\"\n",
    "        if isinstance(content_dict, dict) or isinstance(content_dict, list):\n",
    "            semaphore = asyncio.Semaphore(self.max_concurrent_tasks)\n",
    "\n",
    "            async def sem_task(content):\n",
    "                async with semaphore:\n",
    "                    return await self.atomic_embedder(\n",
    "                        content, vector_field_name, embedding_field_name\n",
    "                    )\n",
    "\n",
    "            tasks = [sem_task(content) for content in content_dict]\n",
    "            results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "        else:\n",
    "            logger.error(\n",
    "                f\"Content is not a dictionary or list, but {type(content_dict)}\"\n",
    "            )\n",
    "            raise TypeError(\n",
    "                f\"Content is not a dictionary or list, but {type(content_dict)}\"\n",
    "            )\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The object to embed the documents\n",
    "hash_docs = document_to_hash(chunks_docs, added_metadata, static_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_embedder = DocumentEmbedder(open_ai_embedder, max_concurrent_tasks=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_hash_docs = await doc_embedder.full_embedder(\n",
    "    hash_docs,\n",
    "    vector_field_name,\n",
    "    embedding_field_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(embedded_hash_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_hash_docs[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, List, Dict, Optional\n",
    "from urllib.parse import quote\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redis_url_params = {\n",
    "    \"redis_host\": redis_host,\n",
    "    \"redis_port\": redis_port,\n",
    "    \"redis_database\": redis_database,\n",
    "    \"redis_password\": redis_password,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redis_url_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_redis_url(\n",
    "    redis_host: str,\n",
    "    redis_port: int,\n",
    "    redis_database: str,\n",
    "    redis_password: Union[str, None],\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Create a redis uri from given args\n",
    "    redis://[username:user_pwd@]name_of_host [:port_number_of_redis_server] [/DB_Name]\n",
    "    redis://[[username]:[password]]@localhost:6379/0\n",
    "    \"\"\"\n",
    "    if not redis_password:\n",
    "        redis_url = f\"redis://{redis_host}:{redis_port}/{redis_database}\"\n",
    "        # print(f\"Redis url is {redis_url}\")\n",
    "\n",
    "    else:\n",
    "        redis_url = f\"redis://default:{quote(redis_password)}@{redis_host}:{redis_port}/{redis_database}\"\n",
    "        # print(f\"Redis url is {redis_url}\")\n",
    "\n",
    "    return redis_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedisVectorStoreRetriever(VectorStoreRetriever):\n",
    "    \"\"\"Retriever for Redis VectorStore.\"\"\"\n",
    "\n",
    "    vectorstore: Any\n",
    "    \"\"\"Redis VectorStore.\"\"\"\n",
    "    search_type: str = \"similarity\"\n",
    "    \"\"\"Type of search to perform. Can be either\n",
    "    'similarity',\n",
    "    'similarity_score_threshold',\n",
    "    \"\"\"\n",
    "\n",
    "    search_kwargs: Dict[str, Any] = {\n",
    "        \"num_results\": 5,\n",
    "        \"score_threshold\": 0.5,\n",
    "        \"extracted_ner_heads\": {},\n",
    "    }\n",
    "\n",
    "    allowed_search_types = [\n",
    "        \"similarity\",\n",
    "        \"similarity_score_threshold\",\n",
    "    ]\n",
    "\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    async def _aget_relevant_documents(\n",
    "        self,\n",
    "        query: str,\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"Get relevant documents from the VectorStore.\"\"\"\n",
    "        if self.search_type == \"similarity\":\n",
    "            # remove score_threshold from search_kwargs\n",
    "            self.search_kwargs.pop(\"score_threshold\", None)\n",
    "            docs = await self.vectorstore.retrieve_from_index(\n",
    "                query, **self.search_kwargs\n",
    "            )\n",
    "\n",
    "        elif self.search_type == \"similarity_score_threshold\":\n",
    "            docs = await self.vectorstore.retrieve_from_index_score_threshold(\n",
    "                query, **self.search_kwargs\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"search_type of {self.search_type} not allowed.\")\n",
    "        return docs\n",
    "\n",
    "    def _get_relevant_documents(\n",
    "        self,\n",
    "        query: str,\n",
    "    ) -> List[Dict]:\n",
    "        return asyncio.run(self._aget_relevant_documents(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedisvlVectorDB:\n",
    "    \"\"\"Redis Vector DB model for storing and retrieving documents.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        static_metadata: list,\n",
    "        added_metadata: list,\n",
    "        embedding_field_name: str,\n",
    "        vector_field_name: str,\n",
    "        embedding_dimension: int,\n",
    "        distance_metric: str,\n",
    "        vector_algo: str,\n",
    "        index_name: str,\n",
    "        doc_embedder: DocumentEmbedder,\n",
    "        llm_re_ranking: bool,\n",
    "        redis_connection: Optional[Redis] = None,\n",
    "        redis_url_params: Optional[Dict[str, Union[str, int]]] = None,\n",
    "    ):\n",
    "        \"\"\"Initialize the Redis Vector DB model.\"\"\"\n",
    "        self.static_metadata = static_metadata\n",
    "        self.added_metadata = added_metadata\n",
    "        self.embedding_field_name = embedding_field_name\n",
    "        self.vector_field_name = vector_field_name\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.distance_metric = distance_metric\n",
    "        self.vector_algo = vector_algo\n",
    "        self.index_name = index_name\n",
    "        self.doc_embedder = doc_embedder\n",
    "        self.llm_re_ranking = llm_re_ranking\n",
    "        self.cross_encoder_reranker = None\n",
    "\n",
    "        if self.llm_re_ranking:\n",
    "            self._initialize_reranker()\n",
    "\n",
    "        if redis_connection:\n",
    "            self.redis_connection = redis_connection\n",
    "        else:\n",
    "            self.redis_connection = None\n",
    "            self.redis_url = (\n",
    "                get_redis_url(**redis_url_params) if redis_url_params else None\n",
    "            )\n",
    "        self.schema = self._create_schema()\n",
    "\n",
    "    def _initialize_reranker(self):\n",
    "        \"\"\"Initialize the cross-encoder reranker for re-ranking.\"\"\"\n",
    "\n",
    "        print(\"Using the cross-encoder reranker for re-ranking.\")\n",
    "        self.cross_encoder_reranker = HFCrossEncoderReranker(\n",
    "            \"cross-encoder/ms-marco-MiniLM-L-6-v2\", limit=5\n",
    "        )\n",
    "\n",
    "    def _create_schema(\n",
    "        self,\n",
    "    ):\n",
    "        \"\"\"Schema creation for vector DB index\"\"\"\n",
    "        fields = (\n",
    "            [\n",
    "                {\"name\": name, \"type\": \"text\"}\n",
    "                for name in self.static_metadata + [self.embedding_field_name]\n",
    "            ]\n",
    "            + [{\"name\": name, \"type\": \"tag\"} for name in self.added_metadata]\n",
    "            + [\n",
    "                {\n",
    "                    \"name\": self.vector_field_name,\n",
    "                    \"type\": \"vector\",\n",
    "                    \"attrs\": {\n",
    "                        \"dims\": self.embedding_dimension,\n",
    "                        \"distance_metric\": self.distance_metric,\n",
    "                        \"algorithm\": self.vector_algo,\n",
    "                        \"datatype\": \"float32\",\n",
    "                    },\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        schema = {\n",
    "            \"index\": {\n",
    "                \"name\": f\"{self.index_name}\",\n",
    "                \"prefix\": uuid.uuid4().hex,\n",
    "            },\n",
    "            \"fields\": fields,\n",
    "        }\n",
    "\n",
    "        return schema\n",
    "\n",
    "    async def get_redis_index(\n",
    "        self,\n",
    "    ):\n",
    "        \"\"\"Get async-redis index connection.\"\"\"\n",
    "        print(f\"Schema:{self.schema}\")\n",
    "\n",
    "        if self.redis_connection is not None:\n",
    "            print(\"Using the existing redis connection\")\n",
    "            index = AsyncSearchIndex.from_dict(self.schema)\n",
    "            await index.set_client(self.redis_connection)\n",
    "            return index\n",
    "        else:\n",
    "            print(\"Establishing redis connection\")\n",
    "            client = Redis.from_url(self.redis_url)\n",
    "            index = AsyncSearchIndex.from_dict(self.schema)\n",
    "            await index.set_client(client)\n",
    "            return index\n",
    "\n",
    "    async def _delete_index(self, index: AsyncSearchIndex):\n",
    "        \"\"\"Delete the index.\"\"\"\n",
    "        try:\n",
    "            await index.delete()\n",
    "        except Exception as e:\n",
    "            print(\"No index present to drop\")\n",
    "\n",
    "    async def drop_index(self):\n",
    "        \"\"\"Drop the index.\"\"\"\n",
    "\n",
    "        index = await self.get_redis_index()\n",
    "        if await index.exists():\n",
    "            await self._delete_index(index)\n",
    "            print(f\"Deleted the index\")\n",
    "            return None\n",
    "\n",
    "    async def _load_docs(self, index: AsyncSearchIndex, embedding_hashes: list):\n",
    "        \"\"\"Load list of dicts to the index.\"\"\"\n",
    "        keys = await index.load(embedding_hashes)\n",
    "        return keys\n",
    "\n",
    "    async def create_redis_index_upload(\n",
    "        self, embedding_hashes: list, drop_index: bool = True\n",
    "    ):\n",
    "        \"\"\"Create a new redis index and upload the documents from the embedding hashes\"\"\"\n",
    "        index = await self.get_redis_index()\n",
    "        if drop_index:\n",
    "            await self._delete_index(index)\n",
    "\n",
    "        # create the index and upload the documents\n",
    "        await index.create(overwrite=True)\n",
    "\n",
    "        keys = await self._load_docs(index, embedding_hashes)\n",
    "        print(f\"created redis index and uploaded {len(keys)} records\")\n",
    "        return keys\n",
    "\n",
    "    async def update_redis_index(self, embedding_hashes: list):\n",
    "        \"\"\"Update the redis index with the new documents.\"\"\"\n",
    "        index = await self.get_redis_index()\n",
    "        if await index.exists():\n",
    "            updated_keys = await self._load_docs(index, embedding_hashes)\n",
    "            print(f\"Appended to redis index and uploaded {len(updated_keys)} records\")\n",
    "            return updated_keys\n",
    "        else:\n",
    "            logger.error(\"No index present to update\")\n",
    "            return None\n",
    "\n",
    "    async def _curate_query(\n",
    "        self,\n",
    "        query: str,\n",
    "        num_results: int,\n",
    "    ):\n",
    "        \"\"\"Curate the query for the search.\"\"\"\n",
    "\n",
    "        query_vector = await self.doc_embedder.content_embedder(query, \"vectors\")\n",
    "        query_search = VectorQuery(\n",
    "            vector=query_vector,\n",
    "            vector_field_name=self.vector_field_name,\n",
    "            return_fields=self.static_metadata\n",
    "            + [\"vector_distance\", self.embedding_field_name]\n",
    "            + self.added_metadata,\n",
    "            num_results=num_results,\n",
    "            return_score=True,\n",
    "        )\n",
    "        return query_search\n",
    "\n",
    "    async def _conditional_filters(self, extracted_ner_heads: dict):\n",
    "        \"\"\"Apply conditional filters on the extracted NER heads.\"\"\"\n",
    "\n",
    "        condition_ls = [\n",
    "            Tag(k.lower()) == v.lower() for k, v in extracted_ner_heads.items()\n",
    "        ]\n",
    "        full_condition = None\n",
    "        for i in range(len(condition_ls)):\n",
    "            if i == 0:\n",
    "                full_condition = condition_ls[i]\n",
    "            else:\n",
    "                full_condition = full_condition & condition_ls[i]\n",
    "\n",
    "        return full_condition\n",
    "\n",
    "    async def _search_index_results(\n",
    "        self,\n",
    "        index: AsyncSearchIndex,\n",
    "        query: str,\n",
    "        num_results: int,\n",
    "        extracted_ner_heads: dict,\n",
    "    ):\n",
    "        \"\"\"Search the index for the query and return the results.\"\"\"\n",
    "\n",
    "        query_search = await self._curate_query(\n",
    "            query=query,\n",
    "            num_results=num_results,\n",
    "        )\n",
    "        if extracted_ner_heads:\n",
    "            print(\n",
    "                \"Extracted NER heads present in the query and applying conditional filters.\"\n",
    "            )\n",
    "            if len(extracted_ner_heads) > 0:\n",
    "                full_condition = await self._conditional_filters(\n",
    "                    extracted_ner_heads=extracted_ner_heads\n",
    "                )\n",
    "                query_search.set_filter(full_condition)\n",
    "\n",
    "        # search the index for the query and return the results\n",
    "        results = await index.query(query_search)\n",
    "        return results\n",
    "\n",
    "    async def _rerank_results(\n",
    "        self,\n",
    "        results: list,\n",
    "        query: str,\n",
    "    ):\n",
    "        \"\"\"Rerank the results using the cross-encoder.\"\"\"\n",
    "\n",
    "        re_rank_raw_docs = [r[\"page_content\"] for r in results]\n",
    "        ids = [{\"id\": r[\"id\"]} for r in results]\n",
    "        re_rank_op = await self.cross_encoder_reranker.arank(query, re_rank_raw_docs)\n",
    "        op_ls = [\n",
    "            {\"content\": op[0][\"content\"], \"score\": op[1]}\n",
    "            for op in zip(re_rank_op[0], re_rank_op[1])\n",
    "        ]\n",
    "        filter_op_ls = list(filter(lambda x: x[\"score\"] > 0, op_ls))\n",
    "        filter_op_ls = list(zip(ids, filter_op_ls))\n",
    "        return filter_op_ls\n",
    "\n",
    "    async def _re_rank_selection(self, reranked_results, results):\n",
    "        \"\"\"Select the ids from the reranked results and filter the results based on the final list.\"\"\"\n",
    "\n",
    "        # selecting ids from the reranked results\n",
    "        final_results_ids = [r[0][\"id\"] for r in reranked_results]\n",
    "        # filter the results based on the final list that we got from reranking results\n",
    "        final_results = [r for r in results if r[\"id\"] in final_results_ids]\n",
    "        return final_results\n",
    "\n",
    "    async def retrieve_from_index(\n",
    "        self,\n",
    "        query: str,\n",
    "        num_results: int,\n",
    "        extracted_ner_heads: dict,\n",
    "    ):\n",
    "        \"\"\"Retrieve the results from the index.\"\"\"\n",
    "\n",
    "        # get the index connection from redis\n",
    "        index = await self.get_redis_index()\n",
    "        # search the index for the query and return the results\n",
    "        redis_results = await self._search_index_results(\n",
    "            index,\n",
    "            query,\n",
    "            num_results,\n",
    "            extracted_ner_heads,\n",
    "        )\n",
    "        if self.llm_re_ranking:\n",
    "            print(\"Reranking the results using the cross-encoder.\")\n",
    "            reranked_results = await self._rerank_results(redis_results, query)\n",
    "\n",
    "            # select the ids from the reranked results and filter the results based on the final list\n",
    "            final_selected_results = await self._re_rank_selection(\n",
    "                reranked_results, redis_results\n",
    "            )\n",
    "            return final_selected_results\n",
    "\n",
    "        return redis_results\n",
    "\n",
    "    async def retrieve_from_index_score_threshold(\n",
    "        self,\n",
    "        query: str,\n",
    "        num_results: int,\n",
    "        extracted_ner_heads: dict,\n",
    "        score_threshold: float,\n",
    "    ):\n",
    "        \"\"\"Retrieve the results from the index based on the score threshold.\"\"\"\n",
    "\n",
    "        retrieved_results = await self.retrieve_from_index(\n",
    "            query,\n",
    "            num_results,\n",
    "            extracted_ner_heads,\n",
    "        )\n",
    "        filtered_results = [\n",
    "            r\n",
    "            for r in retrieved_results\n",
    "            if (1 - float(r[\"vector_distance\"])) > score_threshold\n",
    "        ]\n",
    "\n",
    "        return filtered_results\n",
    "\n",
    "    def as_retriever(self, **kwargs: Any) -> RedisVectorStoreRetriever:\n",
    "        \"\"\"Return a RedisVectorStoreRetriever object.\"\"\"\n",
    "        return RedisVectorStoreRetriever(vectorstore=self, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redis_vector_db = RedisvlVectorDB(\n",
    "    static_metadata,\n",
    "    added_metadata,\n",
    "    embedding_field_name,\n",
    "    vector_field_name,\n",
    "    embedding_dimension,\n",
    "    distance_metric,\n",
    "    vector_algo,\n",
    "    index_name,\n",
    "    doc_embedder,\n",
    "    llm_re_ranking,\n",
    "    redis_connection=None,\n",
    "    redis_url_params=redis_url_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = await redis_vector_db.get_redis_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the index and upload the documents\n",
    "new_keys = await redis_vector_db.create_redis_index_upload(\n",
    "    embedded_hash_docs, drop_index=True\n",
    ")\n",
    "print(len(new_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Update new documents on the same index\n",
    "# update_keys = await redis_vector_db.update_redis_index(embedded_hash_docs)\n",
    "# print(len(update_keys))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retriver from the Redis Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# await redis_vector_db.drop_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Which parties are involved in the MSA contracts?\"\n",
    "# query = \"What is the expected YOE for the role of Strategy Consultant in this agreement?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_op = await redis_vector_db.retrieve_from_index(query, 5, {\"contractor_name\": \"Accenture\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_op2 = await redis_vector_db.retrieve_from_index_score_threshold(query, 5, {\"contractor_name\": \"Accenture\"}, 0.8)\n",
    "# result_op2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = redis_vector_db.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\n",
    "        \"num_results\": 5,\n",
    "        \"score_threshold\": 0.75,\n",
    "        \"extracted_ner_heads\": {},\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_op = await retriever._aget_relevant_documents(\n",
    "    query\n",
    ")  # This is the async method to get the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever._get_relevant_documents(query) # This is the sync method to get the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Langchain Summarizer chain with custom retriever and lecl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import (\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"\n",
    "    You are an AI assistant specialized in answering questions based on the provided documents. Your task is to answer the questions based on the documents provided.\n",
    "    The related documents are provided under the '<<<Documents>>>' context and the question is provided under the '<<<Question>>>' context.\n",
    "    You can use the documents to answer the questions.\n",
    "    The final answer should be a clear and concise answer to the question with the relevant information extracted from the documents provided under the '<<<Documents>>>' context.\n",
    "    The final answer should also be grammatically correct and should be in complete sentences and should be relevant to the question asked.\n",
    "    At the end of the answer, please provide a short and concise summary of the answer.\n",
    "    <<<Documents>>>: Context of the documents provided using which the question should be answered. It contains the relevant information extracted from the documents and the page numbers at the start of each content.Page numbers are also indication of the start of a new document content.\n",
    "    <<<Formating_Instructions>>>: Formatting instructions for the answer.\n",
    "    <<<Question>>>: The question that needs to be answered using the documents provided.\n",
    "    \"\"\"\n",
    "\n",
    "human_message = \"\"\"\n",
    "    <<<Documents>>>:\\n\n",
    "    {context}\n",
    "    <<<Formating_Instructions>>>:\\n\n",
    "    - While summarizing the answer, please ensure that the answer is clear, concise, and relevant to the question asked. The answer should be in complete sentences and should be grammatically correct.\n",
    "    - At the end of every sentence in the final summary, please provide the only the page number from where the information is extracted as citations.\n",
    "    <<<Question>>>:\\n\n",
    "    {question}\n",
    "\n",
    "    Answer: \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_generator(\n",
    "    system_message: str = \"\", human_message: str = \"\"\n",
    ") -> ChatPromptTemplate:\n",
    "    prompt_template = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            SystemMessagePromptTemplate.from_template(system_message),\n",
    "            HumanMessagePromptTemplate.from_template(human_message),\n",
    "        ]\n",
    "    )\n",
    "    return prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qna_prompt = prompt_generator(system_message, human_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    \"\"\"Format the documents for the prompt.\"\"\"\n",
    "    return \"\\n\\n\".join(\n",
    "        \"\\nPageNumber:\" + doc[\"page_number\"] + \"\\nPageContent:\" + doc[\"page_content\"]\n",
    "        for doc in docs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(format_docs(sample_op))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model = get_gpt_model(\n",
    "    azure_deployment=os.getenv(\"CHAT_ENGINE_GPT4_DEPLOYMENT_NAME\"),\n",
    "    model_name=os.getenv(\"CHAT_ENGINE_GPT4_MODEL_NAME\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    openai_api_type=os.getenv(\"OPENAI_API_TYPE\"),\n",
    "    api_version=os.getenv(\"OPENAI_API_VERSION\"),\n",
    "    temperature=0.0,\n",
    "    request_timeout=45,\n",
    "    max_retries=5,\n",
    "    seed=1234,\n",
    "    top_p=0.0001,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | qna_prompt\n",
    "    | llm_model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op = await rag_chain.ainvoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#################-------------------------------------------------------------############################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_unstructured",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
