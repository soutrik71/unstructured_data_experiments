{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "IPython.notebook.set_autosave_interval(300000)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 300 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 300\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/batch/tasks/shared/LS_root/mounts/clusters/copilot-model-run/code/Users/Soutrik.Chowdhury/unstructured_data_experiments\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\n",
    "    \"/mnt/batch/tasks/shared/LS_root/mounts/clusters/copilot-model-run/code/Users/Soutrik.Chowdhury/unstructured_data_experiments\"\n",
    ")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import re\n",
    "import urllib.parse\n",
    "\n",
    "import numpy as np\n",
    "from joblib import delayed, Parallel, parallel_backend\n",
    "from langchain_community.document_loaders import (\n",
    "    CSVLoader,\n",
    "    Docx2txtLoader,\n",
    "    PyMuPDFLoader,\n",
    "    TextLoader,\n",
    "    UnstructuredPowerPointLoader,\n",
    ")\n",
    "\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "from functools import partial\n",
    "from tenacity import retry, stop_after_attempt\n",
    "from typing import Any, Dict, List, Union\n",
    "import asyncio\n",
    "\n",
    "\n",
    "from redisvl.index import SearchIndex\n",
    "from redis import Redis\n",
    "from urllib.parse import quote\n",
    "from redisvl.query import VectorQuery\n",
    "from redisvl.query.filter import Tag\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from redisvl.index import AsyncSearchIndex\n",
    "from redis.asyncio import Redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_file = \"dev.env\"\n",
    "\n",
    "load_dotenv(find_dotenv(env_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenAIEmbeddingFunctions:\n",
    "    \"\"\"\n",
    "    Class to get the OpenAIEmbeddings for embedding documents.\n",
    "    Attributes:\n",
    "        api_key (str): The API key of the OpenAI model.\n",
    "        api_base (str): The API base of the OpenAI model.\n",
    "        api_type (str): The API type of the OpenAI model.\n",
    "        api_version (str): The API version of the OpenAI model.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        api_key: str = os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "        api_base: str = os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "        api_type: str = os.environ.get(\"OPENAI_API_TYPE\"),\n",
    "        api_version: str = os.environ.get(\"OPENAI_API_VERSION\"),\n",
    "        model_name: str = os.environ.get(\"EMBEDDING_ENGINE_ADA_DEPLOYMENT_NAME\"),\n",
    "        model_deployment_name: str = os.environ.get(\"EMBEDDING_ENGINE_ADA_MODEL_NAME\"),\n",
    "    ) -> None:\n",
    "        self.api_key = api_key\n",
    "        self.api_base = api_base\n",
    "        self.api_type = api_type\n",
    "        self.api_version = api_version\n",
    "        self.model_name = model_name\n",
    "        self.model_deployment_name = model_deployment_name\n",
    "\n",
    "    def get_openai_embedder(self):\n",
    "        \"\"\"\n",
    "        Get an instance of OpenAIEmbeddings for embedding documents.\n",
    "\n",
    "        Args:\n",
    "            openai_pkg: The OpenAI package.\n",
    "            model (str, optional): The model name. Defaults to None.\n",
    "            deployment (str, optional): The deployment name. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            An instance of OpenAIEmbeddings for embedding documents.\n",
    "        \"\"\"\n",
    "\n",
    "        return AzureOpenAIEmbeddings(\n",
    "            model=self.model_name,\n",
    "            azure_deployment=self.model_deployment_name,\n",
    "            api_key=self.api_key,\n",
    "            azure_endpoint=self.api_base,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open_ai_embedder = OpenAIEmbeddingFunctions().get_openai_embedder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# op = open_ai_embedder.embed_documents([\"This is a test document.\"])\n",
    "# print(len(op[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# op = await open_ai_embedder.aembed_query(\"This is a test document.\")\n",
    "# print(len(op))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# op = await open_ai_embedder.aembed_documents(\n",
    "#     [\"This is a test document.\", \"This is a test document2\"]\n",
    "# )\n",
    "# print(len(op))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/langchain_unstructured/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import PyPDFium2Loader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "import pickle\n",
    "from redisvl.utils.rerank import HFCrossEncoderReranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = \"data/TDS_and_TCS-rate-chart-2025.pdf\"\n",
    "# loader = PyPDFium2Loader(file_path)\n",
    "\n",
    "# docs = loader.load()\n",
    "\n",
    "# print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs = pickle.load(open(\"extracted_docs/Accenture/extracted_docs.pkl\", \"rb\"))\n",
    "# print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_splitter = RecursiveCharacterTextSplitter(\n",
    "#     chunk_size=1000, chunk_overlap=10)\n",
    "# splits = text_splitter.split_documents(docs)\n",
    "# print(len(splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_splitter = CharacterTextSplitter(\n",
    "#     chunk_size=1000, chunk_overlap=10, separator=\"\\n\\n\"\n",
    "# )  # page break level\n",
    "# splits = text_splitter.split_documents(docs)\n",
    "# print(len(splits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__DOCUMENTS_CHUNKING__\\\n",
    "we have 3 options to solve this problem:\n",
    "* recursively split by some characters to keep the similar content together\n",
    "* splits based on characters (by default \"\\n\\n\") and measure chunk length by number of characters.\n",
    "* Or Keep the document as given by the extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read documents from Documents folder\n",
    "class DynamicDocumentSplitter:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        doc_folder_path: str,\n",
    "        split_type: str,\n",
    "        min_word_count: int,\n",
    "        overlap_fraction: float,\n",
    "        max_word_count: int,\n",
    "    ):\n",
    "\n",
    "        self.doc_folder_path = doc_folder_path\n",
    "        self.split_type = split_type\n",
    "        self.min_word_count = min_word_count\n",
    "        self.overlap_fraction = overlap_fraction\n",
    "        self.max_word_count = max_word_count\n",
    "\n",
    "    def get_document(self):\n",
    "        \"\"\"Get document from the file.\"\"\"\n",
    "        docs = pickle.load(open(self.doc_folder_path, \"rb\"))\n",
    "        return docs\n",
    "\n",
    "    def get_dynamic_chunk_details(self, doc):\n",
    "        \"\"\"Determine dynamic chunk size and overlap for document splitting.\"\"\"\n",
    "        logger.debug(\"Calculating chunk details.\")\n",
    "\n",
    "        def count_func(x):\n",
    "            return len(re.findall(r\"\\w+\", x))\n",
    "\n",
    "        dynamic_chunk_sz = int(\n",
    "            np.mean([count_func(doc_element.page_content) for doc_element in doc])\n",
    "        )\n",
    "\n",
    "        if dynamic_chunk_sz < self.min_word_count:\n",
    "            return {\n",
    "                \"chunk_size\": self.min_word_count,\n",
    "                \"chunk_overlap\": int(self.min_word_count * self.overlap_fraction),\n",
    "            }\n",
    "        elif dynamic_chunk_sz > self.max_word_count:\n",
    "            return {\n",
    "                \"chunk_size\": self.max_word_count,\n",
    "                \"chunk_overlap\": int(self.max_word_count * self.overlap_fraction),\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"chunk_size\": dynamic_chunk_sz,\n",
    "                \"chunk_overlap\": int(dynamic_chunk_sz * self.overlap_fraction),\n",
    "            }\n",
    "\n",
    "    def create_chunks_docs(self, docs, chunk_details):\n",
    "        \"\"\"Create document chunks based on the specified splitting method.\"\"\"\n",
    "        logger.debug(\"Creating document chunks.\")\n",
    "\n",
    "        text_splitter = (\n",
    "            RecursiveCharacterTextSplitter\n",
    "            if self.split_type == \"recursive\"\n",
    "            else CharacterTextSplitter if self.split_type == \"character\" else None\n",
    "        )\n",
    "        if text_splitter is None:\n",
    "            raise ValueError(\"Invalid split type.\")\n",
    "\n",
    "        docs = text_splitter(\n",
    "            chunk_size=chunk_details[\"chunk_size\"],\n",
    "            chunk_overlap=chunk_details[\"chunk_overlap\"],\n",
    "            add_start_index=True,\n",
    "        ).split_documents(docs)\n",
    "\n",
    "        return docs\n",
    "\n",
    "    def get_chunked_docs(\n",
    "        self,\n",
    "    ):\n",
    "        \"\"\"Get chunks for a given file.\"\"\"\n",
    "        org_docs = self.get_document()\n",
    "        if self.split_type == \"pages\":\n",
    "            return org_docs\n",
    "\n",
    "        chunk_details = self.get_dynamic_chunk_details(org_docs)\n",
    "        chunk_docs = self.create_chunks_docs(org_docs, chunk_details)\n",
    "        return chunk_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_folder_path = \"extracted_docs/Accenture/extracted_docs.pkl\"\n",
    "split_type = \"pages\"\n",
    "min_word_count = 1000\n",
    "overlap_fraction = 0.1\n",
    "max_word_count = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_splitter = DynamicDocumentSplitter(\n",
    "    doc_folder_path, split_type, min_word_count, overlap_fraction, max_word_count\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_docs = doc_splitter.get_chunked_docs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO:\n",
    "1. We can have a parallel implementation of the above code to split the documents in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__DOCUMENTS_EMBEDDING__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "added_metadata = [\"contractor_name\", \"file_version\"]\n",
    "static_metadata = [\n",
    "    \"source\",\n",
    "    \"page_number\",\n",
    "    \"file_name\",\n",
    "    \"token_size\",\n",
    "    \"timestamp\",\n",
    "]\n",
    "open_ai_embedder = OpenAIEmbeddingFunctions().get_openai_embedder()\n",
    "vector_field_name = \"embeddings\"\n",
    "embedding_field_name = \"page_content\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_to_hash(\n",
    "    all_docs: list, added_metadata: list, static_metadata: list\n",
    ") -> list:\n",
    "    \"\"\"Convert the documents to a hash for indexing.\"\"\"\n",
    "    hash_docs = [\n",
    "        {\n",
    "            \"page_content\": doc.page_content,\n",
    "        }\n",
    "        | {f\"{meta}\": str(doc.metadata[f\"{meta}\"]).lower() for meta in static_metadata}\n",
    "        | {f\"{meta}\": doc.metadata[f\"{meta}\"].lower() for meta in added_metadata}\n",
    "        for doc in all_docs\n",
    "    ]\n",
    "    return hash_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_docs = document_to_hash(chunks_docs, added_metadata, static_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentEmbedder:\n",
    "    def __init__(self, open_ai_embedder, max_concurrent_tasks: int = 10):\n",
    "        self.open_ai_embedder = open_ai_embedder\n",
    "        self.max_concurrent_tasks = max_concurrent_tasks\n",
    "\n",
    "    async def content_embedder(\n",
    "        self,\n",
    "        content: str,\n",
    "        op_type: str = \"bytes\",\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Embed the content using the OpenAI embeddings to create embeddings for one unit.\n",
    "\n",
    "        Args:\n",
    "            content (str): The content to embed.\n",
    "            op_type (str): The operation type, either 'bytes' or the original array.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The embedding vector.\n",
    "        \"\"\"\n",
    "        embedding_vector = await self.open_ai_embedder.aembed_documents([content])\n",
    "        embd_array = np.array(embedding_vector[0]).astype(np.float32)\n",
    "\n",
    "        if op_type == \"bytes\":\n",
    "            return embd_array.tobytes()\n",
    "\n",
    "        return embd_array\n",
    "\n",
    "    @retry(stop=stop_after_attempt(5))\n",
    "    async def atomic_embedder(\n",
    "        self,\n",
    "        content_dict: dict,\n",
    "        vector_field_name: str,\n",
    "        embedding_field_name: str,\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Embed the content using the OpenAI embeddings to create embeddings for one unit.\n",
    "\n",
    "        Args:\n",
    "            content_dict (dict): The content dictionary.\n",
    "            vector_field_name (str): The name of the vector field.\n",
    "            embedding_field_name (str): The name of the embedding field in the document.\n",
    "\n",
    "        Returns:\n",
    "            dict: The updated content dictionary with the embedding.\n",
    "        \"\"\"\n",
    "        item_content = content_dict[embedding_field_name]\n",
    "        embedding_vector = await self.content_embedder(item_content)\n",
    "        content_dict[vector_field_name] = embedding_vector\n",
    "        return content_dict\n",
    "\n",
    "    async def full_embedder(\n",
    "        self,\n",
    "        content_dict: list,\n",
    "        vector_field_name: str,\n",
    "        embedding_field_name: str,\n",
    "    ) -> list:\n",
    "        \"\"\"\n",
    "        Atomic loading of each content dictionary with the embedding field.\n",
    "        This method is supposed to change as per different use cases.\n",
    "\n",
    "        Args:\n",
    "            content_dict (list): The list of content dictionaries.\n",
    "            vector_field_name (str): The name of the vector field.\n",
    "            embedding_field_name (str): The name of the embedding field in the document.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of content dictionaries with the embedding field updated.\n",
    "        \"\"\"\n",
    "        if isinstance(content_dict, dict) or isinstance(content_dict, list):\n",
    "            semaphore = asyncio.Semaphore(self.max_concurrent_tasks)\n",
    "\n",
    "            async def sem_task(content):\n",
    "                async with semaphore:\n",
    "                    return await self.atomic_embedder(\n",
    "                        content, vector_field_name, embedding_field_name\n",
    "                    )\n",
    "\n",
    "            tasks = [sem_task(content) for content in content_dict]\n",
    "            results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "        else:\n",
    "            logger.error(\n",
    "                f\"Content is not a dictionary or list, but {type(content_dict)}\"\n",
    "            )\n",
    "            raise TypeError(\n",
    "                f\"Content is not a dictionary or list, but {type(content_dict)}\"\n",
    "            )\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_embedder = DocumentEmbedder(open_ai_embedder, max_concurrent_tasks=10)\n",
    "embedded_hash_docs = await doc_embedder.full_embedder(\n",
    "    hash_docs,\n",
    "    vector_field_name,\n",
    "    embedding_field_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedded_hash_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__REDISVL_VECTORDB__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "redis_host = os.environ.get(\"REDIS_HOST\")\n",
    "redis_port = os.environ.get(\"REDIS_PORT\")\n",
    "redis_database = os.environ.get(\"REDIS_DATABASE\")\n",
    "redis_password = os.environ.get(\"REDIS_PASSWORD\")\n",
    "index_name = os.environ.get(\"REDIS_INDEX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_redis_url(\n",
    "    redis_host: str,\n",
    "    redis_port: int,\n",
    "    redis_database: str,\n",
    "    redis_password: Union[str, None],\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Create a redis uri from given args\n",
    "    redis://[username:user_pwd@]name_of_host [:port_number_of_redis_server] [/DB_Name]\n",
    "    redis://[[username]:[password]]@localhost:6379/0\n",
    "    \"\"\"\n",
    "    if not redis_password:\n",
    "        redis_url = f\"redis://{redis_host}:{redis_port}/{redis_database}\"\n",
    "        logger.debug(f\"Redis url is {redis_url}\")\n",
    "\n",
    "    else:\n",
    "        redis_url = f\"redis://default:{quote(redis_password)}@{redis_host}:{redis_port}/{redis_database}\"\n",
    "        logger.debug(f\"Redis url is {redis_url}\")\n",
    "\n",
    "    return redis_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "redis_url = get_redis_url(redis_host, redis_port, redis_database, redis_password)\n",
    "# print(redis_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dimension = 1536\n",
    "distance_metric = \"cosine\"\n",
    "vector_algo = \"hnsw\"\n",
    "drop_index = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_schema(\n",
    "    static_keys: list,\n",
    "    added_metadata: list,\n",
    "    embedding_field_name: str,\n",
    "    vector_field_name: str,\n",
    "    embedding_dimension: int,\n",
    "    distance_metric: str,\n",
    "    vector_algo: str,\n",
    "    index_name: str,\n",
    "):\n",
    "    \"\"\"Schema creation for vector DB index\"\"\"\n",
    "    fields = (\n",
    "        [\n",
    "            {\"name\": name, \"type\": \"text\"}\n",
    "            for name in static_keys + [embedding_field_name]\n",
    "        ]\n",
    "        + [{\"name\": name, \"type\": \"tag\"} for name in added_metadata]\n",
    "        + [\n",
    "            {\n",
    "                \"name\": vector_field_name,\n",
    "                \"type\": \"vector\",\n",
    "                \"attrs\": {\n",
    "                    \"dims\": embedding_dimension,\n",
    "                    \"distance_metric\": distance_metric,\n",
    "                    \"algorithm\": vector_algo,\n",
    "                    \"datatype\": \"float32\",\n",
    "                },\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    schema = {\n",
    "        \"index\": {\n",
    "            \"name\": f\"{index_name}\",\n",
    "            \"prefix\": \"redis_doc\",\n",
    "        },\n",
    "        \"fields\": fields,\n",
    "    }\n",
    "\n",
    "    return schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# schema = create_schema(\n",
    "#     static_metadata,\n",
    "#     added_metadata,\n",
    "#     embedding_field_name,\n",
    "#     vector_field_name,\n",
    "#     embedding_dimension,\n",
    "#     distance_metric,\n",
    "#     vector_algo,\n",
    "#     index_name,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_redis_index():\n",
    "    \"\"\"Get async-redis index connection.\"\"\"\n",
    "    schema = create_schema(\n",
    "        static_metadata,\n",
    "        added_metadata,\n",
    "        embedding_field_name,\n",
    "        vector_field_name,\n",
    "        embedding_dimension,\n",
    "        distance_metric,\n",
    "        vector_algo,\n",
    "        index_name,\n",
    "    )\n",
    "    client = Redis.from_url(redis_url)\n",
    "    index = AsyncSearchIndex.from_dict(schema)\n",
    "    index = await index.set_client(client)\n",
    "\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def delete_index(index):\n",
    "    try:\n",
    "        await index.delete()\n",
    "    except Exception as e:\n",
    "        logger.info(\"No index present to drop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def load_docs(index, embedding_hashes):\n",
    "    \"\"\"Load list of dicts to the index.\"\"\"\n",
    "    keys = await index.load(embedding_hashes)\n",
    "    return keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def create_redis_index_upload(embedding_hashes, drop_index: bool = True):\n",
    "    \"\"\"Create a redis index and upload the documents.\"\"\"\n",
    "    index = await get_redis_index()\n",
    "    if drop_index:\n",
    "        await delete_index(index)\n",
    "\n",
    "    await index.create(overwrite=True)\n",
    "\n",
    "    keys = await load_docs(index, embedding_hashes)\n",
    "    logger.info(f\"created redis index and uploaded {len(keys)} records\")\n",
    "    return keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = await create_redis_index_upload(embedded_hash_docs, drop_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def update_redis_index(embedding_hashes):\n",
    "    \"\"\"Update the redis index with the new documents.\"\"\"\n",
    "    index = await get_redis_index()\n",
    "    if await index.exists():\n",
    "        updated_keys = await load_docs(index, embedding_hashes)\n",
    "        logger.info(f\"Appended to redis index and uploaded {len(updated_keys)} records\")\n",
    "        return updated_keys\n",
    "    else:\n",
    "        logger.error(\"No index present to update\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_keys = await update_redis_index(embedded_hash_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n"
     ]
    }
   ],
   "source": [
    "print(len(updated_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def curate_query(\n",
    "    query: str,\n",
    "    num_results: int,\n",
    "    doc_embedder,\n",
    "    vector_field_name: str,\n",
    "    embedding_field_name: str,\n",
    "    static_metadata: list,\n",
    "    added_metadata: list,\n",
    "):\n",
    "    \"\"\"Curate the query for the search.\"\"\"\n",
    "    query_vector = await doc_embedder.content_embedder(query, \"vectors\")\n",
    "    query_search = VectorQuery(\n",
    "        vector=query_vector,\n",
    "        vector_field_name=vector_field_name,\n",
    "        return_fields=static_metadata\n",
    "        + [\"vector_distance\", embedding_field_name]\n",
    "        + added_metadata,\n",
    "        num_results=num_results,\n",
    "        return_score=True,\n",
    "    )\n",
    "    return query_search\n",
    "\n",
    "\n",
    "async def conditional_filters(extracted_ner_heads: dict):\n",
    "    \"\"\"Apply conditional filters on the extracted NER heads.\"\"\"\n",
    "    condition_ls = [Tag(k.lower()) == v.lower() for k, v in extracted_ner_heads.items()]\n",
    "    full_condition = None\n",
    "    for i in range(len(condition_ls)):\n",
    "        if i == 0:\n",
    "            full_condition = condition_ls[i]\n",
    "        else:\n",
    "            full_condition = full_condition & condition_ls[i]\n",
    "\n",
    "    return full_condition\n",
    "\n",
    "\n",
    "async def search_index_results(\n",
    "    index: AsyncSearchIndex,\n",
    "    query: str,\n",
    "    num_results: int,\n",
    "    extracted_ner_heads: dict,\n",
    "    doc_embedder,\n",
    "    vector_field_name: str,\n",
    "    embedding_field_name: str,\n",
    "    static_metadata: list,\n",
    "    added_metadata: list,\n",
    "):\n",
    "    \"\"\"Search the index for the query and return the results.\"\"\"\n",
    "    query_search = await curate_query(\n",
    "        query=query,\n",
    "        num_results=num_results,\n",
    "        doc_embedder=doc_embedder,\n",
    "        vector_field_name=vector_field_name,\n",
    "        embedding_field_name=embedding_field_name,\n",
    "        static_metadata=static_metadata,\n",
    "        added_metadata=added_metadata,\n",
    "    )\n",
    "    if extracted_ner_heads:\n",
    "        logger.info(\n",
    "            \"Extracted NER heads present in the query and applying conditional filters.\"\n",
    "        )\n",
    "        if len(extracted_ner_heads) > 0:\n",
    "            full_condition = await conditional_filters(\n",
    "                extracted_ner_heads=extracted_ner_heads\n",
    "            )\n",
    "            query_search.set_filter(full_condition)\n",
    "    results = await index.query(query_search)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Which parties are involved in the MSA contracts?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = await search_index_results(\n",
    "#     index,\n",
    "#     query,\n",
    "#     5,\n",
    "#     {\"contractor_name\": \"Accenture\", \"file_version\": \"original\"},\n",
    "#     doc_embedder,\n",
    "#     vector_field_name,\n",
    "#     embedding_field_name,\n",
    "#     static_metadata,\n",
    "#     added_metadata,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/langchain_unstructured/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "cross_encoder_reranker = HFCrossEncoderReranker(\n",
    "    \"cross-encoder/ms-marco-MiniLM-L-6-v2\", limit=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def rerank_results(\n",
    "    results: list,\n",
    "    query: str,\n",
    "    cross_encoder_reranker,\n",
    "):\n",
    "    \"\"\"Rerank the results using the cross-encoder.\"\"\"\n",
    "\n",
    "    re_rank_raw_docs = [r[\"page_content\"] for r in results]\n",
    "    ids = [{\"id\": r[\"id\"]} for r in results]\n",
    "    re_rank_op = await cross_encoder_reranker.arank(query, re_rank_raw_docs)\n",
    "    op_ls = [\n",
    "        {\"content\": op[0][\"content\"], \"score\": op[1]}\n",
    "        for op in zip(re_rank_op[0], re_rank_op[1])\n",
    "    ]\n",
    "    filter_op_ls = list(filter(lambda x: x[\"score\"] > 0, op_ls))\n",
    "    filter_op_ls = list(zip(ids, filter_op_ls))\n",
    "    return filter_op_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reranked_results = await rerank_results(results, query, cross_encoder_reranker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reranked_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def re_rank_selection(reranked_results, results):\n",
    "    \"\"\"Select the ids from the reranked results and filter the results based on the final list.\"\"\"\n",
    "    # selecting ids from the reranked results\n",
    "    final_results_ids = [r[0][\"id\"] for r in reranked_results]\n",
    "    # filter the results based on the final list that we got from reranking results\n",
    "    final_results = [r for r in results if r[\"id\"] in final_results_ids]\n",
    "    return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_selected_results = await re_rank_selection(reranked_results, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_selected_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def retrieve_from_index(\n",
    "    query: str,\n",
    "    num_results: int,\n",
    "    extracted_ner_heads: dict,\n",
    "    llm_re_ranking: bool,\n",
    "    cross_encoder_reranker,\n",
    "    doc_embedder,\n",
    "    vector_field_name: str,\n",
    "    embedding_field_name: str,\n",
    "    static_metadata: list,\n",
    "    added_metadata: list,\n",
    "):\n",
    "    \"\"\"Retrieve the results from the index.\"\"\"\n",
    "    # get the index connection from redis\n",
    "    index = await get_redis_index()\n",
    "    # search the index for the query and return the results\n",
    "    redis_results = await search_index_results(\n",
    "        index,\n",
    "        query,\n",
    "        num_results,\n",
    "        extracted_ner_heads,\n",
    "        doc_embedder,\n",
    "        vector_field_name,\n",
    "        embedding_field_name,\n",
    "        static_metadata,\n",
    "        added_metadata,\n",
    "    )\n",
    "    if llm_re_ranking:\n",
    "        reranked_results = await rerank_results(\n",
    "            redis_results, query, cross_encoder_reranker\n",
    "        )\n",
    "        final_selected_results = await re_rank_selection(\n",
    "            reranked_results, redis_results\n",
    "        )\n",
    "        return final_selected_results\n",
    "    return redis_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_results = await retrieve_from_index(\n",
    "    query,\n",
    "    5,\n",
    "    {\"contractor_name\": \"Accenture\", \"file_version\": \"original\"},\n",
    "    True,\n",
    "    cross_encoder_reranker,\n",
    "    doc_embedder,\n",
    "    vector_field_name,\n",
    "    embedding_field_name,\n",
    "    static_metadata,\n",
    "    added_metadata,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'redis_doc:5ba8cca85e3b488992fee7d171db5472',\n",
       "  'vector_distance': '0.17741048336',\n",
       "  'source': 'accenture__original__msa completely signed accenture',\n",
       "  'page_number': '2',\n",
       "  'file_name': 'msa completely signed accenture',\n",
       "  'token_size': '564',\n",
       "  'timestamp': '2024-08-15 15:07:21.922368',\n",
       "  'page_content': 'THIS AGREEMENT (or \"MSA\") is made on [date]\\n\\nBETWEEN:\\n\\n(1) ANHEUSER-BUSCH INBEV PROCUREMENT GMBH of Suurstoffi 22, 6343 Rotkreuz, Switzerland (\"AB InBev Procurement\"); and\\n\\n(2) AMBEV LUXEMBOURG SARL SENNINGERBERG, RISCH BRANCH, a Swiss branch of a Luxembourg company which has its registered office at Suurstoffi 22, Rotkreuz, 6343, Switzerland (\"AmBev Luxembourg\"),\\n\\n(collectively the \"Lead Customer\"); and\\n\\n(3) ACCENTURE AG, which has its registered address at Fraumuensterstrasse 16, 8001 Zurich, Switzerland (the \"Lead Supplier\").\\n\\nTHE PARTIES AGREE as follows:\\n\\n1 OBJECTIVES AND PRINCIPLES\\n\\n1.1 Objectives - The objective of this Agreement is to create a contracting framework for the Services performed for the ABI Group by the Supplier Group.\\n\\n1.2 Principles\\n\\n(a) The MSA provides a set of guidelines against which the members of the Supplier Group should seek to make their future offers to perform Services.\\n\\n(b) This Agreement is a master agreement negotiated between the parties on behalf of their respective groups, which is intended to have equivalent master services agreements entered into between the Lead Customer and other suppliers (whereby the Lead Customer may contract on behalf of the ABI Group for the provision of similar services from other suppliers), will assist the Lead Customer and Customers in comparing proposals from different suppliers in response to RFPs issued by the ABI Group for Services, and should speed up the SOW contracting process.\\n\\n(c) The MSA and its equivalent master services agreements entered into with other suppliers should enable the Lead Customer to track the level of compliance of a supplier and its Affiliates to the ABI Group\\'s preferred contracting framework across the world.\\n\\n(d) However, each member of the ABI Group may decide what is better for a particular deal and can, subject to clause 3.2, agree with the Supplier, in an SOW, to overwrite specifics of the MSA.\\n\\n1.3 MSA & SOW\\n\\n(a) The Lead Parties\\' intention is to incorporate and standardise as much of the commercial and legal terms between the ABI Group and the Supplier Group applying to the provision of Services as is reasonable.\\n\\n(b) While specific terms may, subject to clause 3.2, be overridden by the individual SOWs it is the Lead Parties\\' intention that this is the exception rather than the rule, especially with the legal terms.\\n\\n(c) Each SOW will be a standalone contract, entered into between a member of the ABI Group and a member of the Supplier Group within the framework created by this MSA,',\n",
       "  'contractor_name': 'accenture',\n",
       "  'file_version': 'original'},\n",
       " {'id': 'redis_doc:88e6a5eae466402d8e087f421062e00a',\n",
       "  'vector_distance': '0.202511787415',\n",
       "  'source': 'accenture__original__msa completely signed accenture',\n",
       "  'page_number': '90',\n",
       "  'file_name': 'msa completely signed accenture',\n",
       "  'token_size': '388',\n",
       "  'timestamp': '2024-08-15 15:12:56.760965',\n",
       "  'page_content': '```\\nDocuSign Envelope ID: 31EF68C8-71A9-4F6A-95F4-4239336148A6\\n\\n(b) Scope\\n\\nThe Contract Management Level will meet quarterly to manage the business partnership and review MSA usage. The Carry Management Level will carry out the following activities:\\n\\n(i) review the list of newly signed SOWs under the MSA and their level of compliance with the principles of the MSA;\\n(ii) review the list of SOWs coming to the end of their SOW Terms during the period;\\n(iii) review the revenue evolution;\\n(iv) review of issues arising under existing SOWs;\\n(v) identify opportunities for amendment to the MSA to be presented to the Executive Level;\\n(vi) presentation of findings of reviews, benchmarking studies, and audits to the Executive Level; and\\n(vii) Any other matters which require Contract Management Level activity.\\n\\n(c) Membership\\n\\nThe Program Management Level comprises managers appointed by the Lead Customer and the Lead Supplier as follows:\\n\\nLead Customer Representation          Lead Supplier Representation\\nCustomer Global Contract Manager      Supplier Account Manager\\nCustomer Global Procurement Manager   Service Contract Executive\\n\\n(d) Reporting\\n\\nThe Lead Supplier will provide the following reports:\\n\\n(i) revenue per month of all active SOWs during the period, with split by ABI Group zone;\\n(ii) list of newly signed SOWs during the period, with split by ABI Group zone;\\n(iii) for each newly signed SOW during the period, the Lead Supplier will list the gaps versus the principles of the MSA, with split by ABI Group zone;\\n(iv) list of all SOWs coming to the end of their SOW Terms during the period, with split by ABI Group zone; and\\n(v) value of Service Credits (arising; and becoming irrecoverable) during the period, with split by ABI Group zone.\\n\\n91\\n```',\n",
       "  'contractor_name': 'accenture',\n",
       "  'file_version': 'original'}]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedisvlVectorDB:\n",
    "    def __init__(\n",
    "        self,\n",
    "        static_keys: list,\n",
    "        added_metadata: list,\n",
    "        embedding_field_name: str,\n",
    "        vector_field_name: str,\n",
    "        embedding_dimension: int,\n",
    "        distance_metric: str,\n",
    "        vector_algo: str,\n",
    "        index_name: str,\n",
    "        doc_embedder,\n",
    "        llm_re_ranking: bool,\n",
    "        redis_connection=None,\n",
    "    ):\n",
    "        self.static_keys = static_keys\n",
    "        self.added_metadata = added_metadata\n",
    "        self.embedding_field_name = embedding_field_name\n",
    "        self.vector_field_name = vector_field_name\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.distance_metric = distance_metric\n",
    "        self.vector_algo = vector_algo\n",
    "        self.index_name = index_name\n",
    "        self.doc_embedder = doc_embedder\n",
    "        if llm_re_ranking:\n",
    "            self.cross_encoder_reranker = HFCrossEncoderReranker(\n",
    "                \"cross-encoder/ms-marco-MiniLM-L-6-v2\", limit=5\n",
    "            )\n",
    "        self.llm_re_ranking = llm_re_ranking\n",
    "        if redis_connection is not None:\n",
    "            self.redis_url = get_redis_url(\n",
    "                redis_host, redis_port, redis_database, redis_password\n",
    "            )\n",
    "        self.redis_connection = redis_connection\n",
    "\n",
    "    def create_schema(\n",
    "        self,\n",
    "    ):\n",
    "        \"\"\"Schema creation for vector DB index\"\"\"\n",
    "        fields = (\n",
    "            [\n",
    "                {\"name\": name, \"type\": \"text\"}\n",
    "                for name in self.static_keys + [self.embedding_field_name]\n",
    "            ]\n",
    "            + [{\"name\": name, \"type\": \"tag\"} for name in self.added_metadata]\n",
    "            + [\n",
    "                {\n",
    "                    \"name\": self.vector_field_name,\n",
    "                    \"type\": \"vector\",\n",
    "                    \"attrs\": {\n",
    "                        \"dims\": self.embedding_dimension,\n",
    "                        \"distance_metric\": self.distance_metric,\n",
    "                        \"algorithm\": self.vector_algo,\n",
    "                        \"datatype\": \"float32\",\n",
    "                    },\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        schema = {\n",
    "            \"index\": {\n",
    "                \"name\": f\"{self.index_name}\",\n",
    "                \"prefix\": \"redis_doc\",\n",
    "            },\n",
    "            \"fields\": fields,\n",
    "        }\n",
    "\n",
    "        return schema\n",
    "\n",
    "    async def get_redis_index(\n",
    "        self,\n",
    "    ):\n",
    "        \"\"\"Get async-redis index connection.\"\"\"\n",
    "        schema = create_schema()\n",
    "        if self.redis_connection is not None:\n",
    "            index = AsyncSearchIndex.from_dict(schema)\n",
    "            index = await index.set_client(self.redis_connection)\n",
    "            return index\n",
    "        else:\n",
    "            client = Redis.from_url(self.redis_url)\n",
    "            index = AsyncSearchIndex.from_dict(schema)\n",
    "            index = await index.set_client(client)\n",
    "            return index\n",
    "\n",
    "    async def _delete_index(self, index: AsyncSearchIndex):\n",
    "        try:\n",
    "            await index.delete()\n",
    "        except Exception as e:\n",
    "            logger.info(\"No index present to drop\")\n",
    "\n",
    "    async def drop_index(self):\n",
    "        \"\"\"Drop the index.\"\"\"\n",
    "\n",
    "        index = await self.get_redis_index()\n",
    "        if await index.exists():\n",
    "            await self._delete_index(index)\n",
    "            logger.info(f\"Deleted the index\")\n",
    "            return None\n",
    "\n",
    "    async def _load_docs(self, index: AsyncSearchIndex, embedding_hashes: list):\n",
    "        \"\"\"Load list of dicts to the index.\"\"\"\n",
    "        keys = await index.load(embedding_hashes)\n",
    "        return keys\n",
    "\n",
    "    async def create_redis_index_upload(\n",
    "        self, embedding_hashes: list, drop_index: bool = True\n",
    "    ):\n",
    "        \"\"\"Create a new redis index and upload the documents from the embedding hashes\"\"\"\n",
    "        index = await self.get_redis_index()\n",
    "        if drop_index:\n",
    "            await self._delete_index(index)\n",
    "\n",
    "        await index.create(overwrite=True)\n",
    "\n",
    "        keys = await self._load_docs(index, embedding_hashes)\n",
    "        logger.info(f\"created redis index and uploaded {len(keys)} records\")\n",
    "        return keys\n",
    "\n",
    "    async def update_redis_index(self, embedding_hashes: list):\n",
    "        \"\"\"Update the redis index with the new documents.\"\"\"\n",
    "        index = await get_redis_index()\n",
    "        if await index.exists():\n",
    "            updated_keys = await load_docs(index, embedding_hashes)\n",
    "            logger.info(\n",
    "                f\"Appended to redis index and uploaded {len(updated_keys)} records\"\n",
    "            )\n",
    "            return updated_keys\n",
    "        else:\n",
    "            logger.error(\"No index present to update\")\n",
    "            return None\n",
    "\n",
    "    async def _curate_query(\n",
    "        self,\n",
    "        query: str,\n",
    "        num_results: int,\n",
    "    ):\n",
    "        \"\"\"Curate the query for the search.\"\"\"\n",
    "\n",
    "        query_vector = await self.doc_embedder.content_embedder(query, \"vectors\")\n",
    "        query_search = VectorQuery(\n",
    "            vector=query_vector,\n",
    "            vector_field_name=self.vector_field_name,\n",
    "            return_fields=self.static_metadata\n",
    "            + [\"vector_distance\", self.embedding_field_name]\n",
    "            + self.added_metadata,\n",
    "            num_results=num_results,\n",
    "            return_score=True,\n",
    "        )\n",
    "        return query_search\n",
    "\n",
    "    async def _conditional_filters(self, extracted_ner_heads: dict):\n",
    "        \"\"\"Apply conditional filters on the extracted NER heads.\"\"\"\n",
    "\n",
    "        condition_ls = [\n",
    "            Tag(k.lower()) == v.lower() for k, v in extracted_ner_heads.items()\n",
    "        ]\n",
    "        full_condition = None\n",
    "        for i in range(len(condition_ls)):\n",
    "            if i == 0:\n",
    "                full_condition = condition_ls[i]\n",
    "            else:\n",
    "                full_condition = full_condition & condition_ls[i]\n",
    "\n",
    "        return full_condition\n",
    "\n",
    "    async def search_index_results(\n",
    "        self,\n",
    "        index: AsyncSearchIndex,\n",
    "        query: str,\n",
    "        num_results: int,\n",
    "        extracted_ner_heads: dict,\n",
    "    ):\n",
    "        \"\"\"Search the index for the query and return the results.\"\"\"\n",
    "\n",
    "        query_search = await self._curate_query(\n",
    "            query=query,\n",
    "            num_results=num_results,\n",
    "        )\n",
    "        if extracted_ner_heads:\n",
    "            logger.info(\n",
    "                \"Extracted NER heads present in the query and applying conditional filters.\"\n",
    "            )\n",
    "            if len(extracted_ner_heads) > 0:\n",
    "                full_condition = await self._conditional_filters(\n",
    "                    extracted_ner_heads=extracted_ner_heads\n",
    "                )\n",
    "                query_search.set_filter(full_condition)\n",
    "\n",
    "        # search the index for the query and return the results\n",
    "        results = await index.query(query_search)\n",
    "        return results\n",
    "\n",
    "    async def _rerank_results(\n",
    "        self,\n",
    "        results: list,\n",
    "        query: str,\n",
    "    ):\n",
    "        \"\"\"Rerank the results using the cross-encoder.\"\"\"\n",
    "\n",
    "        re_rank_raw_docs = [r[\"page_content\"] for r in results]\n",
    "        ids = [{\"id\": r[\"id\"]} for r in results]\n",
    "        re_rank_op = await self.cross_encoder_reranker.arank(query, re_rank_raw_docs)\n",
    "        op_ls = [\n",
    "            {\"content\": op[0][\"content\"], \"score\": op[1]}\n",
    "            for op in zip(re_rank_op[0], re_rank_op[1])\n",
    "        ]\n",
    "        filter_op_ls = list(filter(lambda x: x[\"score\"] > 0, op_ls))\n",
    "        filter_op_ls = list(zip(ids, filter_op_ls))\n",
    "        return filter_op_ls\n",
    "\n",
    "    async def _re_rank_selection(self, reranked_results, results):\n",
    "        \"\"\"Select the ids from the reranked results and filter the results based on the final list.\"\"\"\n",
    "\n",
    "        # selecting ids from the reranked results\n",
    "        final_results_ids = [r[0][\"id\"] for r in reranked_results]\n",
    "        # filter the results based on the final list that we got from reranking results\n",
    "        final_results = [r for r in results if r[\"id\"] in final_results_ids]\n",
    "        return final_results\n",
    "\n",
    "    async def retrieve_from_index(\n",
    "        self,\n",
    "        query: str,\n",
    "        num_results: int,\n",
    "        extracted_ner_heads: dict,\n",
    "    ):\n",
    "        \"\"\"Retrieve the results from the index.\"\"\"\n",
    "\n",
    "        # get the index connection from redis\n",
    "        index = await self.get_redis_index()\n",
    "        # search the index for the query and return the results\n",
    "        redis_results = await self.search_index_results(\n",
    "            index,\n",
    "            query,\n",
    "            num_results,\n",
    "            extracted_ner_heads,\n",
    "        )\n",
    "        if self.llm_re_ranking:\n",
    "            logger.info(\"Reranking the results using the cross-encoder.\")\n",
    "            reranked_results = await self._rerank_results(redis_results, query)\n",
    "\n",
    "            # select the ids from the reranked results and filter the results based on the final list\n",
    "            final_selected_results = await self._re_rank_selection(\n",
    "                reranked_results, redis_results\n",
    "            )\n",
    "            return final_selected_results\n",
    "\n",
    "        return redis_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain_core.documents import Document\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extracted_ner_heads = {\"SUPPLIER\": \"HINDALCO\", \"YEAR\": \"2024\", \"TIMEPERIOD\": \"Q3\"}\n",
    "\n",
    "# num_results = 10\n",
    "\n",
    "# # # Query the index\n",
    "# res = asyncio.run(\n",
    "#     search_index(\n",
    "#         query=question,\n",
    "#         extracted_ner_heads=extracted_ner_heads,\n",
    "#         num_results=num_results,\n",
    "#     )\n",
    "# )\n",
    "# print(\"RESULTS :::::::::::\", res)\n",
    "\n",
    "# prompt = ChatPromptTemplate.from_messages(\n",
    "#     [\n",
    "#         (\n",
    "#             \"system\",\n",
    "#             \"Summarize the provided context into no more than 4 bullet points by focusing into the total revenue generated in the timeperiod, and on the sources of revenue and the factors influencing its fluctuations. Revenue is the total income generated by a company from its business activities, including sales of goods or services. Provide a comprehensive breakdown of the revenue streams, such as product lines, geographic regions, segments, or any other relevant categories. Critically assess the trends in revenue over different periods, highlighting any notable increases or decreases and their underlying causes. Analyze the drivers of revenue growth or decline, including market conditions, changes in consumer behavior, pricing strategies, product launches, acquisitions, or divestitures. Emphasize the key factors contributing to changes in revenue, such as increased demand, product mixes, expansion into new markets, or regulatory changes. Identify any risks or challenges affecting revenue generation, such as competition, supply chain disruptions, economic downturns, or technological shifts. Present insights into the company's revenue diversification, highlighting areas of strength and potential areas for improvement. Ensure each bullet point provide clear and concise explanations for any significant fluctuations in revenue, ensuring relevance and specificity to the company's operations :\\n\\n{context}\",\n",
    "#         )\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# chain = create_stuff_documents_chain(logical_model, prompt)\n",
    "\n",
    "# # List of Document objects\n",
    "# documents = [Document(page_content=item[\"page_content\"]) for item in res]\n",
    "# source = [item[\"source\"] for item in res]\n",
    "# # print(\"SOURCE :::::::::::\", source)\n",
    "# file_names = list(set([os.path.basename(s) for s in source]))\n",
    "\n",
    "# summary = chain.invoke({\"context\": documents})\n",
    "\n",
    "# print(\"Summary:\")\n",
    "# print(summary)\n",
    "# print(\"SOURCE :\", file_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_unstructured",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
